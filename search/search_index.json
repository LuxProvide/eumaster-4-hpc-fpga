{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course description","text":"<p>This course is a live workout session provided in the context of the EUMaster4HPC project, participants will explore how to program High-Performance FPGA cards using the Intel DPC++ compiler, a SYCL implementation, to facilitate heterogeneous device programming. </p> <p>Field-Programmable Gate Arrays (FPGAs) are integrated circuits that can be configured by users post-manufacturing. They consist of a network of programmable logic blocks and reconfigurable interconnects, enabling the creation of custom digital circuits. Traditionally, FPGAs have been programmed using Hardware Description Languages (HDLs) such as Verilog and VHDL. However, designing high-performance accelerators with these languages demands extensive knowledge and experience in hardware design. </p> <p>By leveraging higher-level abstractions like SYCL and OpenCL\u2014C/C++-based programming models familiar to software developers\u2014developers can generate hardware kernels through an offline compiler. This significantly simplifies FPGA programming, reducing development time compared to HDL, which typically involves more complex coding, simulation, and debugging processes. </p> <p>Following an introduction to Meluxina\u2019s FPGA cards and an overview of SYCL programming using the OneAPI software development toolkit, an hands-on session will be proposed to students.  </p>"},{"location":"#agenda","title":"Agenda:","text":""},{"location":"#why-using-fpgas-as-hardware-accelerators-ha","title":"Why using FPGAs as Hardware Accelerators (HA)?","text":"<ul> <li> <p>Customizability and Reconfigurability: Unlike CPUs and GPUs, which have fixed architectures, FPGAs can be programmed to create custom hardware configurations. This allows for the optimization of specific algorithms or processes, which can be particularly beneficial for quantum simulations, where different algorithms might benefit from different hardware optimizations.</p> </li> <li> <p>Parallel Processing: FPGAs can be designed to handle parallel computations natively using pipeline parallelism.</p> </li> <li> <p>Low Latency and High Throughput: FPGAs can provide lower latency than CPUs and GPUs because they can be programmed to execute tasks without the overhead of an operating system or other software layers. This makes them ideal for real-time processing and simulations.</p> </li> <li> <p>Energy Efficiency: FPGAs can be more energy-efficient than GPUs and CPUs for certain tasks because they can be stripped down to only the necessary components required for a specific computation, reducing power consumption.</p> </li> </ul>"},{"location":"#intel-fpga-sdk-oneapi-for-fpga","title":"Intel\u00ae FPGA SDK &amp; oneAPI for FPGA","text":"<ul> <li>The Intel\u00ae FPGA Software Development Kit (SDK) provides a comprehensive set of development tools and libraries specifically designed to facilitate the design, creation, testing, and deployment of applications on Intel's FPGA hardware. The SDK includes tools for both high-level and low-level programming, including support for hardware description languages like VHDL and Verilog, as well as higher-level abstractions using OpenCL or HLS (High-Level Synthesis). This makes it easier for developers to leverage the power of FPGAs without needing deep expertise in hardware design.</li> </ul> <ul> <li>Intel\u00ae oneAPI is a unified programming model designed to simplify development across diverse computing architectures\u2014CPUs, GPUs, FPGAs, and other accelerators. The oneAPI for FPGA component specifically targets the optimization and utilization of Intel FPGAs. It allows developers to use a single, consistent programming model to target various hardware platforms, facilitating easier code reuse and system integration. oneAPI includes specialized libraries and tools that enable developers to maximize the performance of their applications on Intel FPGAs while maintaining a high level of productivity and portability.</li> </ul> <p>In this course, you will learn to:</p> <ul> <li> <p>How to use Meluxina's FPGA, i.e., Intel\u00ae FPGA</p> </li> <li> <p>How to use fundamentals for SYCL programming</p> </li> </ul> <p>Remark</p> <p>This course is not intended to be exhaustive. In addition, the described tools and features are constantly evolving. We try our best to keep it up to date. </p>"},{"location":"#who-is-the-course-for","title":"Who is the course for ?","text":"<ul> <li> <p>This course is for students, researchers, enginners wishing to discover how to use oneAPI to program FPGA. </p> </li> <li> <p>Participants should still have some experience with modern C++ (e.g., Lambdas, class deduction templates).</p> </li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Data Parallel C++: Programming Accelerated Systems Using C++ and SYCL (second edition)</li> <li>Intel oneAPI DPC++/C++ Compiler Handbook for Intel FPGAs</li> </ul>"},{"location":"#about-this-course","title":"About this course","text":"<p>This course has been developed by the Supercomputing Application Services group at LuxProvide in the context of the EUMaster4HPC project.</p>"},{"location":"compile/","title":"Compiling SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"compile/#setup","title":"Setup","text":"<p>After connecting to one of Meluxina's login node, please clone first the oneAPI-sample repository with the <code>git clone --depth 1 https://github.com/oneapi-src/oneAPI-samples.git</code> in your home folder.</p> <p>FPGA support was removed from the Intel\u00ae oneAPI Toolkits starting 2025.1</p> <p>Altera, originally a chipmaker acquired by Intel in 2015, was spun out as an independent company in February 2024. Now led by Sandra Rivera, it focuses on reconfigurable FPGA chips for data center, cloud    , industrial, and automotive applications.  As a consequence, if you want to use oneAPI 2025.1 examples, you will need to clone <code>https://github.com/altera-fpga/hls-samples</code>. </p> <p>Once the repository cloned, you should see the following hierarchy:</p> <pre><code>tree -d -L 2 oneAPI-samples\noneAPI-samples\n\u251c\u2500\u2500 AI-and-Analytics\n\u2502   \u251c\u2500\u2500 End-to-end-Workloads\n\u2502   \u251c\u2500\u2500 Features-and-Functionality\n\u2502   \u251c\u2500\u2500 Getting-Started-Samples\n\u2502   \u251c\u2500\u2500 images\n\u2502   \u2514\u2500\u2500 Jupyter\n\u251c\u2500\u2500 common\n\u2502   \u2514\u2500\u2500 stb\n\u251c\u2500\u2500 DirectProgramming\n\u2502   \u251c\u2500\u2500 C++\n\u2502   \u251c\u2500\u2500 C++SYCL\n\u2502   \u251c\u2500\u2500 C++SYCL_FPGA\n\u2502   \u2514\u2500\u2500 Fortran\n\u251c\u2500\u2500 Libraries\n\u2502   \u251c\u2500\u2500 oneCCL\n\u2502   \u251c\u2500\u2500 oneDAL\n\u2502   \u251c\u2500\u2500 oneDNN\n\u2502   \u251c\u2500\u2500 oneDPL\n\u2502   \u251c\u2500\u2500 oneMKL\n\u2502   \u2514\u2500\u2500 oneTBB\n\u251c\u2500\u2500 Publications\n\u2502   \u251c\u2500\u2500 DPC++\n\u2502   \u2514\u2500\u2500 GPU-Opt-Guide\n\u251c\u2500\u2500 RenderingToolkit\n\u2502   \u251c\u2500\u2500 GettingStarted\n\u2502   \u2514\u2500\u2500 Tutorial\n\u251c\u2500\u2500 Templates\n\u2502   \u2514\u2500\u2500 cmake\n\u2514\u2500\u2500 Tools\n    \u251c\u2500\u2500 Advisor\n    \u251c\u2500\u2500 ApplicationDebugger\n    \u251c\u2500\u2500 Benchmarks\n    \u251c\u2500\u2500 GPU-Occupancy-Calculator\n    \u251c\u2500\u2500 Migration\n    \u2514\u2500\u2500 VTuneProfiler\n</code></pre> <ul> <li>As you can see Intel provides numerous code samples and examples to help your grasping the power of the oneAPI toolkit.</li> <li>We are going to focus on <code>DirectProgramming/C++SYCL_FPGA</code>.</li> <li> <p>Create a symbolic at the root of your home directory pointing to this folder: <pre><code>ln -s oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/GettingStarted\ntree -d -L 2 GettingStarted\nGettingStarted\n\u251c\u2500\u2500 fast_recompile\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 fpga_compile\n\u2502   \u251c\u2500\u2500 part1_cpp\n\u2502   \u251c\u2500\u2500 part2_dpcpp_functor_usm\n\u2502   \u251c\u2500\u2500 part3_dpcpp_lambda_usm\n\u2502   \u2514\u2500\u2500 part4_dpcpp_lambda_buffers\n\u2514\u2500\u2500 fpga_template\n    \u2514\u2500\u2500 src\n</code></pre></p> </li> <li> <p>The fpga_compile folder provides basic examples to start compiling SYCL C++ code with the DPC++ compiler</p> </li> <li> <p>The fpga_recompile folder show you how to recompile quickly your code without having to rebuild the FPGA image</p> </li> <li> <p>The fpga_template is a starting template project that you can use to bootstrap a project</p> </li> </ul>"},{"location":"compile/#discovering-devices","title":"Discovering devices","text":"<p>Before targeting a specific hardware accelerator, you need to ensure that the SYCL runtime is able to detect it.</p> <p>Commands</p> <pre><code># We need a job allocation on a FPGA node\nsalloc -A &lt;ACCOUNT&gt; -t 48:00:00 -q default -p fpga -N 1\n# Load the staging environment\nmodule load env/staging/2023.1\nmodule load intel-oneapi\nmodule load 520nmx/20.4\n# Check the available devices\nsycl-ls\n</code></pre> <p>Output</p> <pre><code>[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2023.15.3.0.20_160000]\n[opencl:cpu:1] Intel(R) OpenCL, AMD EPYC 7452 32-Core Processor                                3.0 [2023.15.3.0.20_160000]\n[opencl:acc:2] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0) 1.0 [2023.1]\n[opencl:acc:3] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie1) 1.0 [2023.1]\n</code></pre> <p>Note</p> <p>Note that you can use FPGA emulation on a non-FPGA node !!! Meluxina's CPU partition<pre><code>[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2023.15.3.0.20_160000]\n[opencl:cpu:1] Intel(R) OpenCL, AMD EPYC 7H12 64-Core Processor                                3.0 [2023.15.3.0.20_160000]\n</code></pre></p>"},{"location":"compile/#first-code","title":"First code","text":"<p>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src/vector_add.cpp</p> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\n               int len) {\n  for (int idx = 0; idx &lt; len; idx++) {\n    int a_val = vec_a_in[idx];\n    int b_val = vec_b_in[idx];\n    int sum = a_val + b_val;\n    vec_c_out[idx] = sum;\n  }\n}\n\nconstexpr int kVectSize = 256;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int * vec_a = new int[kVectSize];\n    int * vec_b = new int[kVectSize];\n    int * vec_c = new int[kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec_a[i] = i;\n      vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\n      sycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\n      sycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        h.single_task&lt;VectorAddID&gt;([=]() {\n          VectorAdd(&amp;accessor_a[0], &amp;accessor_b[0], &amp;accessor_c[0], kVectSize);\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = vec_a[i] + vec_b[i];\n      if (vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li> <p>The <code>vector_add.cpp</code> source file contains all the necessary to understand how to create a SYCL program</p> </li> <li> <p>lines 4 and 5 are the minimal headers to include in your SYCL program</p> </li> <li> <p>line 9 is a forward declaration of the kernel name</p> </li> <li> <p>lines 11-19 is a function representing our kernel. Note the absence of <code>__kernel</code>, <code>__global</code> as it exists in OpenCL</p> </li> <li> <p>lines 30-36 are pragmas defining whether you want a full compilation, a CPU emulation or the simulator</p> </li> <li> <p>line 39 is the queue creation. The queue is bounded to a device. We will discuss it later in details.</p> </li> <li> <p>lines 41-46 provides debugging information at runtime.</p> </li> <li> <p>lines 48-54 instantiates 3 vectors. <code>vec_a</code> and <code>vec_b</code> are input C++ arrays and are initialized inside the next loop. <code>vec_c</code> is an output C++ array collecting computation results between <code>vec_a</code> and <code>vec_b</code>.</p> </li> <li> <p>lines 60-62 create buffers for each vector and specify their size. The runtime copies the data to the FPGA global memory when the kernel starts</p> </li> <li> <p>line 64 submits a command group to the device queue</p> </li> <li> <p>lines 66-68 relies on accessors to infer data dependencies. \"read_only\" accessors have to wait for data to be fetched. \"no_init\" option indicates to the runtime know that the previous contents of the buffer can be discarded</p> </li> <li> <p>lines 70-73 starts a single tasks (single work-item) and call the kernel function</p> </li> <li> <p>lines 99-105 catch SYCL exceptions and terminate the execution</p> </li> </ul>"},{"location":"compile/#code-synthesis","title":"Code synthesis","text":"Left: Synthesis time (source: wikis.uni-paderborn.de) -- Right: Development workflow <ul> <li> <p>Hardware synthesis can be very long </p> </li> <li> <p>Emulation is a practical way of testing your kernels</p> </li> </ul>"},{"location":"compile/#emulation","title":"Emulation","text":"<ul> <li> <p>FPGA emulation refers to the process of using a software or hardware system to mimic the behaviour of an FPGA device. This is usually done to test, validate, and debug FPGA designs before deploying them on actual hardware. The Intel\u00ae FPGA emulator runs the code on the host cpu.</p> </li> <li> <p>Emulation is crucial to validate the functionality of your kernel design. </p> </li> <li> <p>During emulation, your are not seeking for performance.</p> </li> </ul> <p>Compile for emulation (in one step)</p> <pre><code>icpx -fsycl -fintelfpga -qactypes vector_add.cpp -o vector_add.fpga_emu\n</code></pre> <p>Intel uses the SYCL Ahead-of-time (AoT) compilation which as two steps:</p> <ol> <li> <p>The \"compile\" stage compiles the device code to an intermediate representation (SPIR-V).</p> </li> <li> <p>The \"link\" stage invokes the compiler's FPGA backend before linking.</p> </li> </ol> <p>Two-steps compilation</p> <pre><code># Compile \nicpx -fsycl -fintelfpga -qactypes -o vector_add.cpp.o -c vector_add.cpp\n# Link\nicpx -fsycl -fintelfpga -qactypes vector_add.cpp.o -o vector_add.fpga_emu\n</code></pre> <ul> <li>The compiler option <code>-qactypes</code> informs the compiler to search and include the Algorithmic C (AC) data type folder for header and libs to the AC data types libraries for Field Programmable Gate Array (FPGA) and CPU compilations.</li> <li>The Algorithmic C (AC) datatypes libraries include a numerical set of datatypes and an interface datatype for modelling channels in communicating processes in C++.</li> </ul>"},{"location":"compile/#static-reports","title":"Static reports","text":"<ul> <li> <p>During the process of compiling an FPGA hardware image with the Intel\u00ae oneAPI DPC++/C++ Compiler, various checkpoints are provided at different compilation steps. These steps include object files generation, an FPGA early image object generation, an FPGA image object generation, and finally executables generation. These checkpoints offer the ability to review errors and make modifications to the source code without needing to do a full compilation every time. </p> </li> <li> <p>When you reach the FPGA early image object checkpoint, you can examine the optimization report generated by the compiler. </p> </li> <li> <p>Upon arriving at the FPGA image object checkpoint, the compiler produces a finished FPGA image.</p> </li> </ul> <p>In order to generate the FPGA early image, you will need to add the following option:</p> <ul> <li> <p><code>-Xshardware</code></p> </li> <li> <p><code>-Xstarget=&lt;target&gt;</code> or <code>-Xsboard=&lt;board&gt;</code></p> </li> <li> <p><code>-fsycl-link=early</code></p> </li> </ul> <p>Compile for FPGA early image</p> <pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xsboard=p520_hpc_m210h_g3x16 vector_add.cpp -o vector_add_report.a\n</code></pre> <ul> <li> <p>The <code>vector_add_report.a</code> is not what we target in priority. We target the reports directory <code>vector_add_report.prj</code> which has been created.</p> </li> <li> <p>You can evaluate whether the estimated kernel performance data is satisfactory by going to the /reports/ directory and examining one of the following files related to your application: <li> <p>report.html: This file can be viewed using Internet browsers of your choice</p> </li> <li>.zip: Utilize the Intel\u00ae oneAPI FPGA Reports tool,i.e., <code>fpga_report</code>"},{"location":"compile/#full-compilation","title":"Full compilation","text":"<p>This phase produces the actual FPGA bitstream, i.e., a file containing the programming data associated with your FPGA chip. This file requires the target FPGA platform to be generated and executed. For FPGA programming, the Intel\u00ae oneAPI toolkit requires the Intel\u00ae Quartus\u00ae Prime software to generate this bitstream.</p> <p>Full hardware compilation</p> <pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -DFPGA_HARDWARE vector_add.cpp -o vector_add_report.fpga\n</code></pre> <ul> <li> <p>The compilation will take several hours. Therefore, we strongly advise you to verify your code through emulation first.</p> </li> <li> <p>You can also use the <code>-Xsfast-compile</code> option which offers a faster compile time but reduce the performance of the final FPGA image.</p> </li> </ul>"},{"location":"compile/#fast-recompilation","title":"Fast recompilation","text":"<ul> <li> <p>At first glance having a single source file is not necessarily a good idea when host and device compilation differs so much</p> </li> <li> <p>However, there is two different strategies to deal with it:</p> </li> <li> <p>Use a single source file and add the <code>-reuse-exe</code></p> </li> <li> <p>Separate host and device code compilation in your FPGA project</p> </li> <li> <p>This is up to you to choose the method that suits you the most</p> </li> </ul> <p>Using the <code>-reuse-exe</code> option</p> <p><pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -DFPGA_HARDWARE -reuse-exe=vector_add.fpga vector_add.cpp -o vector_add.fpga\n</code></pre> If only the host code changed since the previous compilation, providing the <code>-reuse-exe=image</code> flag to <code>icpx</code> instructs the compiler to extract the compiled FPGA binary from the existing executable and package it into the new executable, saving the device compilation time.</p> <p>Question</p> <ul> <li>What happens if the vector_add.fpga is missing ?</li> </ul> <p>Separating host and device code</p> <p>Go to the <code>GettingStarted/fpga_recompile</code> folder. It provides an example of separate host and device code The process is similar as the compilation process for OpenCL except that a single tool is used, i.e., <code>icpx</code></p> <ol> <li>Compile the host code: <pre><code>icpx -fsycl -fintelfpga -DFPGA_HARDWARE host.cpp -c -o host.o\n</code></pre></li> <li>Compile the FPGA image: <pre><code>icpx -fsycl -fintelfpga -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -fsycl-link=image kernel.cpp -o dev_image.a\n</code></pre></li> <li>Link both: <pre><code>icpx -fsycl -fintelfpga host.o dev_image.a -o fast_recompile.fpga\n</code></pre></li> </ol>"},{"location":"dpcpp/","title":"Introduction to FPGA programming with Intel\u00ae oneAPI","text":"<p>Intel\u00ae oneAPI is a software development toolkit from Intel designed to simplify the process of developing high-performance applications for various types of computing architecture. It aims to provide a unified and simplified programming model for CPUs, GPUs, FPGAs, and other types of hardware, such as AI accelerators, allowing developers to use a single codebase for multiple platforms.</p> <p>One of the main components of oneAPI is the Data Parallel C++ (DPC++), an open, standards-based language built upon the ISO C++ and SYCL standards. DPC++ extends C++ with features like parallel programming constructs and heterogeneous computing support, providing developers with the flexibility to write code for different types of hardware with relative ease.</p> <p>In addition to DPC++, oneAPI includes a range of libraries designed to optimize specific types of tasks, such as machine learning, linear algebra, and deep learning. These include oneDNN for deep neural networks, oneMKL for math kernel library, and oneDAL for data analytics, among others.</p> <p>It's important to note that Intel oneAPI is part of Intel's broader strategy towards open, standards-based, cross-architecture programming, which is intended to reduce the complexity of application development and help developers leverage the capabilities of different types of hardware more efficiently and effectively.</p> <p>In this documentation, you will explore how to:</p> <ul> <li>Use the DPC++ compiler to create executable for Intel FPGA hardware</li> <li>Discover the SYCL C++ abstraction layer</li> <li>How to move data from and to FPGA hardware</li> <li>Optimize FPGA workflows</li> </ul> <p>In order to get an overview of FPGA computing for the HPC ecosystem, please refer to the following slides.</p>"},{"location":"dpcpp/#what-is-the-intel-oneapi-dpc-compiler","title":"What is the Intel\u00ae oneAPI DPC++ compiler","text":"<p>In heterogenous computing, accelerator devices support the host processor by executing specific portion of code more efficiently. In this context, the Intel\u00ae oneAPI toolkit supports two different approaches for heterogeous computing:</p> <p>1. Data Parallel C++ with SYCL</p> <p>SYCL (Specification for Unified Cross-platform C++) provides a higher-level model for writing standard ISO C++ code that is both performance-oriented and portable across various hardware, including CPUs, GPUs and FPGAs It enables the use of standard C++ with extensions to leverage parallel hardware. Host and kernel code share the same source file. The DPC++ compiler is adding SYCL support on top of the LLVM C++ compiler. DPC++ is distributed with the Intel\u00ae oneAPI toolkit.</p> <p>2. OpenMP for C, C++, and Fortran </p> <p>For more than two decades, OpenMP has stood as a standard programming language, with Intel implementing its 5<sup>th</sup> version. The Intel oneAPI C++ Compiler, which includes support for OpenMP offloading, can be found in the Intel oneAPI Base Toolkit, Intel oneAPI HPC Toolkit, and Intel oneAPI IoT Toolkit. Both the Intel\u00ae Fortran Compiler Classic and the Intel\u00ae Fortran Compiler equipped with OpenMP offload support are accessible through the Intel oneAPI HPC Toolkit. </p> <p>Note: OpenMP is not supported for FPGA devices.</p>"},{"location":"dpcpp/#dpc-is-one-of-the-existing-sycl-implementations","title":"DPC++ is one of the existing SYCL implementations","text":"<ul> <li>Data Parallel C++ is the oneAPI's Implementation of SYCL.</li> </ul> <p>ComputeCpp (codeplay)</p> <p>Support for ComputeCpp will no longer be provided from September 1<sup>st</sup> 2023 (see announce)</p>"},{"location":"dpcpp/#key-features-and-components","title":"Key Features and Components","text":"<ul> <li>Heterogeneous Support: Enables coding for various types of processors within the same program.</li> <li>Performance Optimization: It offers various optimization techniques to ensure code runs as efficiently as possible.</li> <li>Standard Compliance: Aligns with the latest C++ standards, along with the SYCL standard.</li> <li>Debugging and Analysis Tools: Integrates with tools that assist in debugging and analyzing code.</li> <li>Integration with IDEs: Compatible with popular Integrated Development Environments to facilitate a seamless coding experience.</li> <li>Open Source and Community Driven: This promotes collaboration and ensures that the technology stays up to date with industry needs.</li> </ul>"},{"location":"dpcpp/#sycl","title":"SYCL","text":"(source: https://www.khronos.org) <ul> <li> <p>SYCL  abstractions to enable heterogeneous device programming</p> </li> <li> <p>SYCL's stratregy  is to allow different heterogenous devices, e.g., CPUs, GPUs, FPGAs, to be used simultaneously</p> </li> <li> <p>Generic programming with templates and lambda functions</p> </li> </ul>"},{"location":"dpcpp/#intel-oneapi-fpga","title":"Intel\u00ae oneAPI FPGA","text":"<p>SYCL offers APIs and abstractions, but FPGA cards are unique to each vendor, and even within the same vendor, FPGA cards may have diverse capabilities. DPC++ targets Intel\u00ae FPGA cards specifically and extends SYCL's functions. This allows it to leverage the strength of FPGA, all the while maintaining as much generalizability and portability as possible.</p>"},{"location":"dpcpp/#references","title":"References","text":"<ul> <li>Data Parallel C++: Mastering DPC++ for Programming of Heterogeneous Systems using C++ and SYCL</li> <li>Intel\u00ae oneAPI DPC++/C++ Compiler </li> <li>SYCL official documentation</li> </ul>"},{"location":"exercices/","title":"Hands-on session","text":"<ul> <li> <p>This hands-on session is designed to provide you with practical, interactive experience. It allows you to apply theoretical knowledge directly by working with real tools or code, thereby deepening your understanding, building practical skills, and boosting confidence in the subject matter.</p> </li> <li> <p>Using the resources seen this morning, i.e.,</p> <ul> <li>Compiling SYCL programs</li> <li>Developing SYCL programs</li> <li>Reporting &amp; Profiling SYCL programs</li> <li>Optimizing SYCL programs </li> </ul> </li> </ul>"},{"location":"exercices/#access-to-meluxina","title":"Access to MeluXina","text":"<ul> <li> <p>This workshop IS NOT a session on how to connect to MeluXina</p> </li> <li> <p>You SHOULD ALL have access to MeluXina before starting the workshop </p> </li> <li> <p>Please note that if you did not register in time, we WILL NOT onboard you during the workshop</p> </li> <li> <p>You SHOULD all have received a Welcome Email providing all the necessary guidelines to connect MeluXina</p> </li> <li> <p>If you did not setup your connection to MeluXina in time, please follow the documentation or contact the service desk</p> </li> </ul>"},{"location":"exercices/#setup","title":"Setup","text":"<ul> <li>You need first to obtain an interactive job on the fpga partition and load a module <pre><code>salloc -A &lt;ACCOUNT&gt; --reservation=&lt;RESERVATION&gt; -t 00:30:00 -q short -p cpu -N1\nmodule load git-lfs\n</code></pre></li> <li>Clone first the repository and enter the <code>exercices</code> folder <pre><code>git lfs clone --depth 1 https://gitlab.lxp.lu/emmanuel.kieffer/eumaster-4-hpc-fpga.git\ncd eumaster-4-hpc-fpga.git/exercices\n</code></pre></li> </ul> <p>Reservations</p> <p>Please use only the two following reservations which have dedicated resources for this event:</p> <ul> <li> <p><code>#SBATCH --reservation=eumaster4hpc-fpga</code> for accessing the 15 reserved FPGA nodes</p> </li> <li> <p><code>#SBATCH --reservation=eumaster4hpc-cpu</code> for accessing the 30 reserved CPU nodes</p> </li> </ul> <p>While emulation can be done using both reservations, FPGA image provided to you can only be executed using the <code>eumaster4hpc-fpga</code> reservation.</p>"},{"location":"exercices/#exercices","title":"Exercices","text":""},{"location":"exercices/#e01-first-compilation","title":"E01-first-compilation","text":"<p>First compilation</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E01-first-compilation</code></li> <li>Add the minimun required code in <code>src/01-first-compilation.cpp</code> to select the FPGA device and create a sycl queue</li> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\nint main() {\n  bool passed = true;\n  try {\n  /*\n   *\n   *\n   *  ADD YOUR CODE HERE\n   *\n   *\n   *\n   *\n   */ \n   std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E01-first-compilation.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E01-first-compilation.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E01-first-compilation.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E01-first-compilation.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E01-first-compilation.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E01-first-compilation.fpga\n</code></pre></li> </ul>"},{"location":"exercices/#e02-enqueue-kernel","title":"E02-enqueue-kernel","text":"<p>Enqueuing a single task kernel</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E02-enqueue-kernel</code></li> <li>Add the minimun required code in <code>src/02-enqueue-kernel.cpp</code> to enqueue a single-task kernel. The body of the kernel should contain the following: <pre><code>PRINTF(\"Result1: Hello, World!\\n\");\nPRINTF(\"Result2: %%\\n\");\nPRINTF(\"Result3: %d\\n\", x);\nPRINTF(\"Result4: %u\\n\", 123);\nPRINTF(\"Result5: %.2f\\n\", y);\nPRINTF(\"Result6: print slash_n \\\\n \\n\");\nPRINTF(\"Result7: Long: %ld\\n\", 650000L);\nPRINTF(\"Result8: Preceding with blanks: %10d \\n\", 1977);                                                  \nPRINTF(\"Result9: Preceding with zeros: %010d \\n\", 1977);                                                  \nPRINTF(\"Result10: Some different radices: %d %x %o %#x %#o \\n\", 100,                                      \n       100, 100, 100, 100);\nPRINTF(\"Result11: ABC%c\\n\", 'D');\n</code></pre></li> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\nusing namespace sycl;\n\n#define PRINTF(format, ...)                                    \\\n  {                                                            \\\n    static const CL_CONSTANT char _format[] = format;          \\\n    ext::oneapi::experimental::printf(_format, ##__VA_ARGS__); \\\n  }\n\nclass BasicKernel;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    /*\n    *\n    *   Enqueue kernel here\n    *\n    *\n    */\n\n\n\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E02-enqueue-kernel.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E02-enqueue-kernel.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E02-enqueue-kernel.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E02-enqueue-kernel.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E02-enqueue-kernel.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E02-enqueue-kernel.fpga\n</code></pre></li> </ul>"},{"location":"exercices/#e03-transfer-data","title":"E03-transfer-data","text":"<p>Transferring data to the device</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E03-transfer-data</code></li> <li>Transfer the data from <code>host_vec_a</code> and <code>host_vec_b</code> to the device by using the <code>memcpy</code> function</li> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\nusing namespace sycl;\n\n#define PRINTF(format, ...)                                    \\\n  {                                                            \\\n    static const CL_CONSTANT char _format[] = format;          \\\n    ext::oneapi::experimental::printf(_format, ##__VA_ARGS__); \\\n  }\n\nclass BasicKernel;\n\n\nint main() {\n  bool passed = true;\n  const int kVectSize = 256;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    int* host_vec_a = new int[kVectSize];\n    int* host_vec_b = new int[kVectSize];\n    int * vec_a = malloc_device&lt;int&gt;(kVectSize,q);\n    int * vec_b = malloc_device&lt;int&gt;(kVectSize,q);\n    int * vec_c = malloc_device&lt;int&gt;(kVectSize,q);\n\n    for (int i = 0; i &lt; kVectSize; i++) {\n      host_vec_a[i] = i;\n      host_vec_b[i] = (kVectSize - i);\n    }\n\n    /*\n      * Transfer host_vec_a to vec_a on the device\n      * Transfer host_vec_b to vec_b on the device\n      *\n      */\n\n    q.submit([&amp;](handler&amp; h) {\n      h.single_task&lt;BasicKernel&gt;([=]() {\n        for(int i = 0; i &lt; kVectSize; i++){\n          PRINTF(\"vect_a[%d]: %d\\n\",i,vec_a[i]);\n          PRINTF(\"vect_b[%d]: %d\\n\",i,vec_b[i]);\n        }\n      });\n    }).wait();\n\n    delete[] host_vec_a;\n    delete[] host_vec_b;\n    sycl::free(vec_a,q); \n    sycl::free(vec_b,q); \n    sycl::free(vec_c,q); \n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E03-transfer-data.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E03-transfer-data.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E03-transfer-data.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E03-transfer-data.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E03-transfer-data.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E03-transfer-data.fpga\n</code></pre></li> </ul>"},{"location":"exercices/#e04-buffer-transfer","title":"E04-buffer-transfer","text":"<p>Using buffers to transfer data to kernels</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E04-buffer-transfer</code></li> <li>Create the buffers and accessors for the two host array <code>host_vec_a</code> and <code>host_vec_b</code></li> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\nusing namespace sycl;\n\n#define PRINTF(format, ...)                                    \\\n  {                                                            \\\n    static const CL_CONSTANT char _format[] = format;          \\\n    ext::oneapi::experimental::printf(_format, ##__VA_ARGS__); \\\n  }\n\nclass BasicKernel;\n\n\nint main() {\n  bool passed = true;\n  const int kVectSize = 256;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    int* host_vec_a = new int[kVectSize];\n    int* host_vec_b = new int[kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      host_vec_a[i] = i;\n      host_vec_b[i] = (kVectSize - i);\n    }\n\n\n    std::cout &lt;&lt; \"Using buffer version\" &lt;&lt; std::endl;\n    {\n   /*\n    *\n    *  Add your code here\n    *\n    *\n    */\n    }\n\n    delete[] host_vec_a;\n    delete[] host_vec_b;\n\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E04-buffer-transfer.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E04-buffer-transfer.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E04-buffer-transfer.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E04-buffer-transfer.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E04-buffer-transfer.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E04-buffer-transfer.fpga\n</code></pre></li> </ul>"},{"location":"exercices/#e05-dot-product","title":"E05-dot-product","text":"<p>Dot-product kernel</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E05-dot-product</code></li> <li>Create a dot product and call it from the kernel to compute the dot-product between host_vec_a and host_vec_b</li> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\nusing namespace sycl;\n\n\nclass DotProductID;\n\nvoid DotProduct(const double *vec_a_in, const double *vec_b_in, double *vec_c_out,\n               int len) {\n// Define your dot product here\n}\n\n\n\nint main() {\n  bool passed = true;\n  const int kVectSize = 256;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    double* host_vec_a = new double[kVectSize];\n    double* host_vec_b = new double[kVectSize];\n    double* host_vec_c = new double[1];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      host_vec_a[i] = i;\n      host_vec_b[i] = (kVectSize - i);\n    }\n\n\n    std::cout &lt;&lt; \"Using buffer version\" &lt;&lt; std::endl;\n    {\n     sycl::buffer buffer_vec_a{host_vec_a,sycl::range(kVectSize)}; \n     sycl::buffer buffer_vec_b{host_vec_b,sycl::range(kVectSize)};\n     sycl::buffer buffer_vec_c{host_vec_c,sycl::range(1)};\n     q.submit([&amp;](handler&amp; h) {\n       sycl::accessor access_vec_a{buffer_vec_a,h,sycl::read_only}; \n       sycl::accessor access_vec_b{buffer_vec_b,h,sycl::read_only};  \n       sycl::accessor access_vec_c{buffer_vec_c,h,sycl::write_only,sycl::no_init};  \n       h.single_task&lt;DotProductID&gt;([=]() {\n          // Call the the function here\n          DotProduct(&amp;access_vec_a[0],&amp;access_vec_b[0],&amp;access_vec_c[0],kVectSize);\n       });\n     });\n    }\n    // verify that correctness\n    double expected = 0;\n    for (int i = 0; i &lt; kVectSize; i++) {\n      expected += (host_vec_a[i] * host_vec_b[i]);\n    }\n\n    if (*host_vec_c != expected) {\n      std::cout &lt;&lt; \"expected=\" &lt;&lt; expected &lt;&lt; \": result \" &lt;&lt; *host_vec_c &lt;&lt; std::endl;\n      passed = false;\n    }else{\n      std::cout &lt;&lt; \"A.B=\" &lt;&lt; host_vec_c &lt;&lt; std::endl;\n    }\n\n    delete[] host_vec_a;\n    delete[] host_vec_b;\n    delete[] host_vec_c;\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E05-dot-product.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E05-dot-product.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E05-dot-product.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E05-dot-product.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E05-dot-product.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E05-dot-product.fpga\n</code></pre></li> </ul>"},{"location":"exercices/#e06-mat-transpose","title":"E06-mat-transpose","text":"<p>Matrix transpose kernel</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E06-mat-transpose</code></li> <li>Create a data-parallel kernel transposing a NxN matrix</li> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\nusing namespace sycl;\n\n#define PRINTF(format, ...)                                    \\\n  {                                                            \\\n    static const CL_CONSTANT char _format[] = format;          \\\n    ext::oneapi::experimental::printf(_format, ##__VA_ARGS__); \\\n  }\n\nclass MatTransposeID;\n\n\nint main() {\n  bool passed = true;\n  const int kVectSize = 256;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    constexpr size_t N = 512;\n    std::vector&lt;float&gt; mat(N * N);\n\n    std::random_device rd;\n    std::mt19937 mt(rd());\n    std::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n\n    // Generate random values\n    std::generate(mat.begin(), mat.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    std::vector&lt;float&gt; copy_mat(mat);\n\n\n    std::cout &lt;&lt; \"Using buffer version\" &lt;&lt; std::endl;\n    {\n     sycl::buffer&lt;float,2&gt; buffer_mat{mat.data(),sycl::range&lt;2&gt;(N,N)};\n     q.submit([&amp;](handler&amp; h) {\n       sycl::accessor access_mat{buffer_mat,h}; \n       /*\n          *\n          *\n          *\n          *  Define and call the kernel here\n          *\n          *\n          *\n          *\n        */\n     });\n    }\n\n\n    // verify that correctness\n    for (int i = 0; i &lt; N; i++) {\n      for (int j = 0; j &lt; N; j++){\n          if (mat[i*N+j] != copy_mat[j*N+i]) {\n            std::cout &lt;&lt; \"expected=\" &lt;&lt; copy_mat[j*N+i] &lt;&lt; \": result \" &lt;&lt;  mat[i*N+j] &lt;&lt; std::endl;\n            passed = false;\n            break;\n          }\n      }\n    }\n\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E06-mat-transpose.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E06-mat-transpose.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E06-mat-transpose.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E06-mat-transpose.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E06-mat-transpose.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E06-mat-transpose.fpga\n</code></pre></li> </ul>"},{"location":"exercices/#e07-shared-memory","title":"E07-shared-memory","text":"<p>Shared memory example</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E07-shared-memory</code></li> <li>The operation that generates a matrix from two vectors is commonly referred to as the outer product (also known as the tensor product or dyadic product) of two vectors.</li> <li>For two vectors, a and b, the outer product results in a matrix where each element is the product of the corresponding components of the vectors.</li> <li> <p>If:</p> <ul> <li> <p>a is an \\((N \\times 1)\\) column vector: \\(a = \\begin{bmatrix}                                                      a_1 \\\\                                                      a_2 \\\\                                                     \\vdots \\\\                                                      a_N \\end{bmatrix}\\)</p> </li> <li> <p>b is an (\\(1 \\times N)\\)  row vector: \\(b = \\begin{bmatrix} b_1 &amp; b_2 &amp; . . .  &amp; b_N \\end{bmatrix}\\)</p> </li> </ul> </li> <li> <p>Then the outer product of a and b, denoted by \\( a \\otimes b \\), results in an \\( m \\times n \\) matrix:</p> \\[a \\otimes b = \\begin{bmatrix} a_1 b_1 &amp; a_1 b_2 &amp; \\cdots &amp; a_1 b_n \\\\ a_2 b_1 &amp; a_2 b_2 &amp; \\cdots &amp; a_2 b_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_m b_1 &amp; a_m b_2 &amp; \\cdots &amp; a_m b_n \\end{bmatrix}\\] </li> <li> <p>This matrix has the element (\\(i, j\\)) equal to the product \\(a_i \\times b_j\\).</p> </li> <li>Compute the outer-product by completing the kernel and use shared memory / group memory to minimize access to the device global memory</li> </ul> <p></p> <ul> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\nusing namespace sycl;\n\n#define PRINTF(format, ...)                                    \\\n  {                                                            \\\n    static const CL_CONSTANT char _format[] = format;          \\\n    ext::oneapi::experimental::printf(_format, ##__VA_ARGS__); \\\n  }\n\nclass VecProductPID;\n\n\nint main() {\n  bool passed = true;\n  const int kVectSize = 256;\n  const int blockSize = 16;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    float* host_vec_a = new float[kVectSize];\n    float* host_vec_b = new float[kVectSize];\n    float* host_vec_c = new float[kVectSize*kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      host_vec_a[i] = i;\n      host_vec_b[i] = (kVectSize - i);\n    }\n\n\n    std::cout &lt;&lt; \"Using buffer version\" &lt;&lt; std::endl;\n    {\n     sycl::buffer buffer_vec_a{host_vec_a,sycl::range(kVectSize)}; \n     sycl::buffer buffer_vec_b{host_vec_b,sycl::range(kVectSize)};\n     sycl::buffer&lt;float,2&gt; buffer_vec_c{host_vec_c,sycl::range&lt;2&gt;(kVectSize,kVectSize)};\n     q.submit([&amp;](handler&amp; h) {\n       sycl::accessor access_vec_a{buffer_vec_a,h,sycl::read_only}; \n       sycl::accessor access_vec_b{buffer_vec_b,h,sycl::read_only};  \n       sycl::accessor access_vec_c{buffer_vec_c,h,sycl::write_only,sycl::no_init};  \n       sycl::local_accessor&lt;float,1&gt; shared_mem{blockSize, h};\n       h.parallel_for&lt;VecProductPID&gt;(sycl::nd_range&lt;1&gt;({kVectSize,blockSize}),[=](sycl::nd_item&lt;1&gt; item){ \n         int m = item.get_global_id()[0];\n         int i = item.get_local_id()[0];\n         for (int p = 0; p &lt; kVectSize/blockSize; p++) {\n            /*\n            *\n            * Add your code here\n            *\n            */\n         }\n       });\n     });\n    }\n\n\n    // verify that correctness\n    for (int i = 0; i &lt; kVectSize; i++) {\n      for (int j =0; j &lt; kVectSize; j++){\n          float expected = host_vec_a[i] * host_vec_b[j];\n          if (expected != host_vec_c[i*kVectSize+j]) {\n            std::cout &lt;&lt; \"expected=\" &lt;&lt; expected &lt;&lt; \": result \" &lt;&lt; host_vec_c &lt;&lt; std::endl;\n            passed = false;\n            break;\n          }\n      }\n    }\n\n    delete[] host_vec_a;\n    delete[] host_vec_b;\n    delete[] host_vec_c;\n\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E07-shared-memory.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E07-shared-memory.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E07-shared-memory.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E07-shared-memory.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E07-shared-memory.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E07-shared-memory.fpga\n</code></pre></li> </ul>"},{"location":"exercices/#e08-pipe","title":"E08-pipe","text":"<p>Fibonnaci ping-pong</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E08-pipe</code></li> <li>Using two kernels and pipes, compute the Fibonnaci sequence using both kernels by exchanging everytime the value computing by one kernel with the other one. </li> <li>Fibonacci Formula:</li> <li>\\(F(n)\\) is the nth Fibonacci number</li> <li>\\(F(0) = 0\\) and \\(F(1) = 1\\) are the base cases</li> <li>\\(F(n) = F(n-1) + F(n-2)\\) </li> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\nusing namespace sycl;\n\n#define PRINTF(format, ...)                                    \\\n  {                                                            \\\n    static const CL_CONSTANT char _format[] = format;          \\\n    ext::oneapi::experimental::printf(_format, ##__VA_ARGS__); \\\n  }\n\nclass PingID;\nclass PongID;\n\nint main() {\n  bool passed = true;\n  const int N = 40;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n\n    std::cout &lt;&lt; \"Using USM device\" &lt;&lt; std::endl;\n    using pipeA = ext::intel::pipe&lt;      \n                class Ping2Pong,\n                long,           \n                1&gt;;             \n\n    using pipeB = ext::intel::pipe&lt;      \n                class Pong2Ping, \n                long,            \n                1&gt;;              \n     q.submit([&amp;](handler&amp; h) {\n       h.single_task&lt;PingID&gt;([=](){ \n        /*\n        *\n        * Add the code for the Ping kernel\n        *\n        *\n        */\n       });\n     });\n\n     q.submit([&amp;](handler&amp; h) {\n       h.single_task&lt;PongID&gt;([=](){ \n        /*\n        *\n        * Add the code for the Pong kernel\n        *\n        *\n        */\n       });\n     });\n\n     q.wait();\n\n     //verify that correctness\n     //long host_vec[N];\n     //q.memcpy(host_vec, device_vec, N * sizeof(long)).wait();\n     //for (int k = 0 ; k &lt; N ; k++){\n     //     std::cout &lt;&lt;\"FIB(\"&lt;&lt;k+1&lt;&lt;\")=\" &lt;&lt; host_vec[k] &lt;&lt; std::endl;\n     //}\n     //\n     //if (host_vec[N-1] != fib_40) {\n     //  std::cout &lt;&lt; \"expected=\" &lt;&lt; fib_40 &lt;&lt; \": result \" &lt;&lt; host_vec[N-1] &lt;&lt; std::endl;\n     //  passed = false;\n     //}\n\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E08-pipe.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E08-pipe.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E08-pipe.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E08-pipe.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E08-pipe.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E08-pipe.fpga\n</code></pre></li> </ul>"},{"location":"exercices/#e09-multi-device","title":"E09-multi-device","text":"<p>Multi-device computations</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E09-multi-device</code></li> <li>You are given an host_array and needs to transfer the first half into device 1 and the second part to device 2</li> <li>The first device will add 1 to each value of the first half while the second one will add 2 to each value of the second half </li> <li>Finally, retrieve from the two half from each device and update the original host_array</li> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\nusing namespace sycl;\n\n#define PRINTF(format, ...)                                    \\\n  {                                                            \\\n    static const CL_CONSTANT char _format[] = format;          \\\n    ext::oneapi::experimental::printf(_format, ##__VA_ARGS__); \\\n  }\n\nclass FirstDeviceID;\nclass SecondDeviceID;\n\nint main() {\n  constexpr int N = 1000;\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n\n    #if FPGA_HARDWARE\n    /*\n      *\n      *\n      * Define the context and the queue here\n      *\n      */\n    #else\n    sycl::queue q0 (selector);\n    sycl::queue q1 (selector);\n    #endif\n\n\n    /*\n      *\n      *\n      * Get the device and print their names\n      *\n      *\n      */\n\n\n    int* host_array = new int[N];\n    std::memset(host_array, 0, sizeof(int)*N);\n    /*\n      *\n      * Setup two array on both device global memories with size N/2\n      * Transfer the first half of the host array on the first device\n      * and the second half on the second array\n      */\n\n    std::cout &lt;&lt; \"Using USM device\" &lt;&lt; std::endl;\n    {\n\n    q0.submit([&amp;](handler&amp; h) {\n       h.single_task&lt;FirstDeviceID&gt;([=](){ \n          for(unsigned int i = 0; i &lt; N/2;i++)\n            device_part1[i]+=1;\n       });\n     });\n\n   q1.submit([&amp;](handler&amp; h) {\n       h.single_task&lt;SecondDeviceID&gt;([=](){ \n          for(unsigned int i = 0; i &lt; N/2;i++)\n            device_part2[i]+=2;\n       });\n     });\n    }\n\n    q0.wait();\n    q1.wait();\n\n    /*\n      *\n      * Retireve the two half and update the host array\n      *\n      */\n\n     for (int k = 0 ; k &lt; N/2 ; k++){\n        if(k&lt;N/2 &amp;&amp; host_array[k]!=1){\n          std::cout&lt;&lt;\"Expected host_array=1 but received host_array=\"&lt;&lt;host_array[k]&lt;&lt;std::endl;\n          passed=false;\n          break;\n        }\n        if(k&gt;=N/2 &amp;&amp; host_array[k]!=2){\n          std::cout&lt;&lt;\"Expected host_array=2 but received host_array=\"&lt;&lt;host_array[k]&lt;&lt;std::endl;\n          passed=false;\n          break;\n        }\n     }\n\n    delete[] host_array;\n    /*\n    * release memory on devices\n    */\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E09-multi-device.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX &amp;&amp; make fpga_emu\n./E09-multi-device.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E09-multi-device.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL  &amp;&amp; make fpga_emu\n./E09-multi-device.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E09-multi-device.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E09-multi-device.fpga\n</code></pre>  Question : Why do you still get a warning for alignment ?</li> </ul>"},{"location":"exercices/#e10-convolution","title":"E10-convolution","text":"<p>Convolution</p> InstructionCode descriptionEmulation (test + solution)Hardware (Solution) <ul> <li>Go to <code>./exercices/E10-convolution</code></li> <li>Create a ndrange sycl kernel to apply a convolution kernel on a png image</li> </ul> <p></p> <ul> <li>Execute your code with the FPGA emulator</li> <li>Execute the FPGA image in the <code>fpga_image</code> folder</li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;fstream&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n\n#ifdef __SYCL_DEVICE_ONLY__\n#define CL_CONSTANT __attribute__((opencl_constant))\n#else\n#define CL_CONSTANT\n#endif\n\n#define pi 3.14159265359\n#define KERNEL_SIZE 3\n\nclass ConvolutionID;\n\nusing namespace sycl;\n\nfloat* load_image(const char* im_path, int* Nx, int* Ny){\n       cv::Mat image = cv::imread(im_path, cv::IMREAD_GRAYSCALE);\n\n       if (image.empty()) {\n           std::cerr &lt;&lt; \"Failed to load image!\" &lt;&lt; std::endl;\n           return nullptr;\n       }\n\n       // Convert to float (optional: normalize to [0,1])\n       cv::Mat imageFloat;\n       image.convertTo(imageFloat, CV_32F); // normalize to 0-1 range\n\n       // Allocate and copy to float*\n       *Nx = imageFloat.rows;\n       *Ny = imageFloat.cols;\n       int size = imageFloat.rows * imageFloat.cols;\n       float* buffer = new float[size];\n       std::memcpy(buffer, imageFloat.ptr&lt;float&gt;(), size * sizeof(float));\n\n       return buffer;\n}\n\nvoid save_image(const char* im_path,float* buffer, const int Nx, const int Ny){\n\n        // Create a CV_32F Mat from the buffer (no copy, wraps memory)\n        cv::Mat floatImage(Nx, Ny, CV_32F, buffer);\n\n        // Convert to 8-bit grayscale for saving or displaying\n        cv::Mat grayImage;\n        floatImage.convertTo(grayImage, CV_8U); // scale from [0,1] \u2192 [0,255]\n\n        // Save or display\n        cv::imwrite(im_path, grayImage);\n}\n\nint main(int argc, char* argv[]) {\n\n  // Check if exactly one argument is passed (excluding program name)\n  if (argc != 2) {\n      std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;filename&gt;\\n\";\n      return 1;\n  }\n  const char* filename = argv[1];\n  std::ifstream file(filename);\n\n  // Check if the file can be opened\n  if (!file) {\n      std::cerr &lt;&lt; \"Error: '\" &lt;&lt; filename &lt;&lt; \"' is not a valid file.\\n\";\n      return EXIT_FAILURE;\n  }\n\n  int Nx;\n  int Ny;\n  float* buffer = load_image(filename,&amp;Nx,&amp;Ny);\n  float* bufferf = (float*)malloc(sizeof(float)*Nx*Ny);\n  std::cout&lt;&lt;\"The image has \"&lt;&lt; Nx &lt;&lt;\" rows and \"&lt;&lt; Ny &lt;&lt;\" cols\"&lt;&lt;std::endl; \n\n  float kernel[(KERNEL_SIZE*KERNEL_SIZE)]={0, 1,0,\n                                           1,-4,1,\n                                           0, 1,0};\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n      #if FPGA_SIMULATOR\n          auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n      #elif FPGA_HARDWARE\n          auto selector = sycl::ext::intel::fpga_selector_v;\n      #else  // #if FPGA_EMULATOR\n          auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n      #endif\n\n    sycl::queue queue (selector);\n\n    auto device = queue.get_device();\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    /*\n     *  1. Create buffers \n     *  2. Create the kernel and call it using the queue\n     *  Hint: Try to use some the optimization seen in the course\n     */\n\n  std::cout&lt;&lt; \" Saving image  \" &lt;&lt; std::endl;\n  save_image(\"output.png\",bufferf,Nx,Ny);\n  free(buffer);\n  free(bufferf);\n\n\n\n\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>To execute your code, uncomment the following lines in <code>launcher_E10-convolution.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\necho \"Building\"\ncmake .. -DBUILD=EX -DUSER_FLAGS=\"-lopencv_core -lopencv_imgcodecs -lopencv_highgui -lopencv_imgproc\" &amp;&amp; make fpga_emu\n./E10-convolution.fpga_emu\n</code></pre></li> <li>To test the solution, uncomment the following lines in <code>launcher_E10-convolution.sh</code> <pre><code>echo \"Create building directory\"\nmkdir -p build &amp;&amp; find build -mindepth 1 -delete &amp;&amp; cd build\ncmake .. -DPASSWORD=\"XXXXXX\" -DBUILD=SOL -DUSER_FLAGS=\"-lopencv_core -lopencv_imgcodecs -lopencv_highgui -lopencv_imgproc\"  &amp;&amp; make fpga_emu\n./E10-convolution.fpga_emu\n</code></pre></li> </ul> <ul> <li>To run the fpga image, uncomment the following lines in <code>launcher_E10-convolution.sh</code></li> <li>Make sure that the header <code>#SBATCH -p fpga</code> is present <pre><code>export JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\ncd fpga_image\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./E10-convolution.fpga\n</code></pre></li> </ul>"},{"location":"intro/","title":"Introduction to FPGA computing for the HPC ecosystem","text":""},{"location":"intro/#field-programmable-gate-array-fpga","title":"Field Programmable Gate Array (FPGA)","text":"<p> An FPGA (Field-Programmable Gate Array) is an integrated circuit designed to be configured by the user after manufacturing. It consists of an array of programmable logic blocks and a hierarchy of reconfigurable interconnects, allowing users to create custom digital circuits. FPGAs are known for their flexibility, enabling rapid prototyping and implementation of complex functions in hardware, making them suitable for applications in telecommunications, automotive, aerospace, and various other fields where custom and High-Performance Computing is needed.</p> <ul> <li>FPGAs are a cheaper off-the-shelf alternative</li> <li>Grid of configurable logic composed of:<ul> <li>Logic Element (LEs) or Adaptive Logic Modules (ALMs)</li> <li>Programmable switches</li> <li>Digital Signal Processing blocks</li> <li>RAM blocks</li> <li>etc \u2026</li> </ul> </li> </ul>"},{"location":"intro/#applications","title":"Applications","text":""},{"location":"intro/#fpga-vendors","title":"FPGA vendors","text":"<ul> <li> <p>Two major FPGA vendors:</p> <ul> <li>Intel Altera</li> <li>Xillinx AMD</li> </ul> </li> <li> <p>Intel acquired Altera in 2015</p> </li> <li> <p>Xillinx is solely focusing on the FPGA market while Intel is a sum of many parts</p> </li> <li> <p>Both profiles are very interesting for heterogeneous computing</p> </li> <li> <p>Among the others:</p> <ul> <li>Lattice Semiconductor</li> <li>QuickLogic</li> <li>Microchip Technology</li> <li>Achronix</li> <li>Efinix</li> </ul> </li> </ul>"},{"location":"intro/#fpga-cards","title":"FPGA cards","text":"<p>FPGA Development Boards and HPC (High-Performance Computing) FPGA Cards serve different purposes and have distinct characteristics:</p> <ol> <li> <p>FPGA Development Boards are primarily designed for learning, prototyping, and small-scale projects. They typically feature user-friendly interfaces, a variety of I/O options, and often include additional components like sensors, buttons, and displays. These boards are intended for engineers, students, and hobbyists to develop and test FPGA-based designs.</p> </li> <li> <p>HPC FPGA Cards, on the other hand, are specialized for High-Performance Computing tasks. These cards are optimized for integration into data centers and High-Performance Computing environments. They focus on maximizing computational power, energy efficiency, and data throughput. HPC FPGA cards are usually designed to be mounted in servers or workstations, and they often support advanced features like high-speed memory interfaces and network connectivity.</p> </li> </ol> <p>Difference Between FPGA Development Boards and HPC FPGA Cards</p> FPGA Development BoardsHPC FPGA Cards <ul> <li>Purpose: Primarily used for prototyping, learning, and development purposes.</li> <li>Design: These boards typically come with various interfaces (like HDMI, USB, Ethernet) and peripherals (like buttons, LEDs, and sensors) to facilitate easy testing and development.</li> <li>Flexibility: They offer a broad range of input/output options to support diverse projects and experiments.</li> <li>Cost: Generally more affordable than HPC FPGA cards due to their focus on versatility and accessibility.</li> <li>Target Audience: Suitable for students, hobbyists, and engineers working on initial project phases or small-scale applications.</li> <li>Programming languages: VHDL, Verilog, System Verilog.</li> <li>Specifications: <ul> <li>Logic Cells: 33,280 </li> <li>Block RAM: 1,800 Kbits</li> <li>External memory: None</li> </ul> </li> </ul> <p></p> <ul> <li>Purpose: Designed for High-Performance Computing (HPC) applications, focusing on accelerating compute-intensive tasks.</li> <li>Design: Typically more powerful, with higher logic capacity, memory, and bandwidth capabilities. They often come with specialized cooling solutions and are designed to be mounted in server racks.</li> <li>Performance: Optimized for tasks such as data center operations, machine learning, financial modeling, and large-scale scientific computations.</li> <li>Cost: Generally more expensive due to their advanced features and high-performance capabilities.</li> <li>Target Audience: Aimed at professionals in industries requiring significant computational power, such as data scientists, researchers, and engineers in High-Performance Computing sectors.</li> <li>Programming languages: HLS: C/C++, python --  HDL: VHDL, Verilog, System Verilog.</li> <li>Specifications:<ul> <li>Logic Cells: 2,073,000 </li> <li>Block RAM: 239.5 MiB</li> <li>External memory: 16GB HBM2</li> </ul> </li> </ul> <p></p>"},{"location":"intro/#hdl-vs-hls","title":"HDL vs HLS","text":"<ul> <li> <p>Hardware Description Language require a more detailed specification of the hardware, providing a gate-level or Register Transfer Level (RTL) description. They require knowledge of the specific hardware constructs, like registers, flip-flops, etc. </p> <ul> <li>Productivity: typically takes more time as developers have to manually describe the low-level hardware details. This can result in more control and optimization but is generally more time-consuming.</li> <li>Flexibility &amp; Optimization: allow developers to describe hardware at a more granular level, there is usually greater opportunity for manual optimization of the design.</li> <li>Learning curve require a deeper understanding of hardware concepts. Thus, there's a steeper learning curve, but it can provide more expertise in hardware design.</li> <li>Use cases is used for more traditional hardware design, where control over implementation details and optimizations is critical.</li> </ul> </li> <li> <p>High-Level Synthesis allows designers to describe hardware using high-level programming languages like C, C++, or SystemC. This means that HLS works at a higher level of abstraction, where developers can describe algorithms or logic without specifying the exact hardware details speeding up design. C/C++ code/kernels are translated to HDL using an offline compiler.  </p> <ul> <li>Productivity: offer faster development time since engineers can write code using familiar programming paradigms. Automated synthesis tools then translate the high-level code into RTL, allowing quicker prototyping.</li> <li>Flexibility &amp; Optimization:  can accelerate development but it often provides less control over the final hardware implementation. This might result in less efficient utilization of FPGA resources or higher latency compared to hand-crafted RTL code.</li> <li>Learning curve has a lower learning curve for software engineers or those familiar with C/C++. This makes it more accessible to developers who might not have a hardware background.</li> <li>Use cases is often preferred for algorithm development, data flow designs, and when a software prototype exists that needs to be converted into hardware.</li> </ul> </li> </ul>"},{"location":"intro/#hls-and-kernel-based-programming-from-opencl-to-sycl","title":"HLS and Kernel-based programming: from OpenCL to SYCL","text":""},{"location":"intro/#opencl-a-low-level-api","title":"OpenCL: a Low-level API","text":"<ul> <li> <p>OpenCL is widely used throughout the industry</p> </li> <li> <p>Kernel-based programming </p> </li> <li> <p>Many silicon vendors ship OpenCL with their processors, including GPUs, DSPs and FPGAs</p> </li> </ul> <p></p>"},{"location":"intro/#sycl-a-high-level-c-abstraction","title":"SYCL: a High-level C++ abstraction","text":"<ul> <li> <p>Full coverage for all OpenCL features</p> </li> <li> <p>Interop to enable existing OpenCL code with SYCL</p> </li> <li> <p>Single-source compilation</p> </li> <li> <p>Automatic scheduling of data movement</p> </li> </ul>"},{"location":"intro/#architecture-of-an-fpga","title":"Architecture of an FPGA","text":"<ul> <li> <p>The differences between Instruction Set Architecture (ISA) for CPUs and Spatial Architecture for FPGAs lie in how they process instructions and handle computation:</p> </li> <li> <p>ISA for CPUs: Sequential, control-flow-oriented, with a fixed architecture using a predefined set of instructions. Suitable for general-purpose tasks.</p> </li> <li> <p>Spatial Architecture for FPGAs: Parallel, data-flow-oriented, with a customizable architecture that can be tailored for specific high-performance tasks. Suitable for specialized, parallelizable workloads.</p> </li> </ul> <p>Difference between Instruction Set architecture and Spatial architecture</p> Instruction Set ArchitectureSpatial Architecture <ul> <li>Made for general-purpose computation: hardware is constantly reused </li> <li>Workflow constrained by a set of pre-defined units (Control Units, ALUs, registers)</li> <li>Data/Register size are fixed</li> <li>Different instruction executed in each clock cycle : temporal execution </li> </ul> <ul> <li>Keep only what it needs -- the hardware can be reconfigured</li> <li>Specialize everything by unrolling the hardware: spatial execution</li> <li>Each operation uses a different hardware region</li> <li>The design can take more space than the FPGA offers </li> </ul> <p> </p> <ul> <li> <p>The most obvious source of parallelism for FPGA is pipelining by inserting registers to store each operation output and keep all hardware unit busy. </p> </li> <li> <p>Pipelining parallelism has therefore many stages. </p> </li> <li> <p>If you don't have enough work to fill the pipeline, then the efficiency is very low.</p> </li> <li> <p>The authors of the DPC++ book have illustrated it perfectly in Chapter 17.</p> </li> </ul> <p>Vectorization</p> <p>Vectorization is also possible but is not the main source of parallelism but help designing efficient pipeline. Since hardware can be reconfigured at will. The offline compiler can design N-bits Adders, multipliers which simplify greatly vectorization. In fact, the offline compiler vectorizes your design automatically if possible.</p>"},{"location":"intro/#fpga-parallelism-pipelining","title":"FPGA parallelism: pipelining","text":"<p>Pipelining (see FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits)</p> <ul> <li>Pipelining is a design technique used in synchronous digital circuits to increase maximum frequency (fMAX).</li> <li>This technique involves adding registers to the critical path, reducing the amount of logic between each register.</li> <li>Reducing logic between registers decreases execution time, enabling an increase in fMAX.</li> <li>The critical path is the path between two consecutive registers that has the highest latency, meaning it\u2019s where operations take the longest to complete.</li> <li>Pipelining is especially effective for processing a stream of data.</li> <li>In a pipelined circuit, different stages can process different data inputs within the same clock cycle.</li> <li>This design improves data processing throughput.</li> </ul> <p></p> <p>Maximum Frequency (fMAX)</p> <ul> <li>The fMAX of a digital circuit is its highest possible clock frequency, determining the maximum rate for updating register outputs. </li> <li>This speed is constrained by the physical propagation delay of the signal across the combinational logic between consecutive register stages.</li> <li>The delay is affected by the complexity of the combinational logic in the path, and the path with the greatest number of logic elements and highest delay sets the speed limit for the entire circuit, often known as the critical path. </li> <li>The fMAX is the reciprocal of this critical path delay, and having a high fMAX is desirable as it leads to better performance when there are no other restrictions. </li> </ul> <p>Occupancy</p> <ul> <li>The occupancy of a datapath at a specific moment signifies the fraction of the datapath filled with valid data.</li> <li>When looking at a circuit's execution of a program, the occupancy is the mean value from the beginning to the end of the program's run.</li> <li>Parts of the datapath that are unoccupied are commonly called bubbles, akin to a CPU's no-operation (no-ops) instructions, which don't influence the final output.</li> <li>Minimizing these bubbles leads to greater occupancy. If there are no other hindrances, optimizing the occupancy of the datapath will boost throughput.  Occupancy: \u2156=40% </li> </ul>"},{"location":"intro/#meluxina-bittware-520n-mx-fpgas","title":"MeluXina Bittware 520N-MX FPGAs","text":"<p>Each of the 20 MeluXina FPGA compute nodes comprise two BittWare 520N-MX FPGAs based on the Intel Stratix 10 FPGA chip. Designed for compute acceleration, the 520N-MX are PCIe boards featuring Intel\u2019s Stratix 10 MX2100 FPGA with integrated HBM2 memory. The size and speed of HBM2 (16GB at up to 512GB/s) enables acceleration of memory-bound applications. Programming with high abstraction C, C++, and OpenCLis possible through an specialized board support package (BSP) for the Intel OpenCL SDK. For more details see the dedicated BittWare product page.</p> <p> <p>Intel Stratix 520N-MX Block Diagram. </p> <p>The Bittware 520N-MX cards have the following specifications:</p> <ol> <li>FPGA: Intel Stratix 10 MX with MX2100 in an F2597 package, 16GBytes on-chip High Bandwidth Memory (HBM2) DRAM, 410 GB/s (speed grade 2).</li> <li>Host interface:    x16 Gen3 interface direct to FPGA, connected to PCIe hard IP.</li> <li>Board Management Controller<ul> <li>FPGA configuration and control</li> <li>Voltage, current, temperature monitoring</li> <li>Low bandwidth BMC-FPGA comms with SPI link</li> </ul> </li> <li>Development Tools<ul> <li>Application development: supported design flows - Intel FPGA OpenCL SDK, Intel High-Level Synthesis (C/C++) &amp; Quartus Prime Pro (HDL, Verilog, VHDL, etc.)</li> <li>FPGA development BIST - Built-In Self-Test with source code (pinout, gateware, PCIe driver &amp; host test application)</li> </ul> </li> </ol> <p>The FPGA cards are not directly connected to the MeluXina ethernet network. The FPGA compute nodes are linked into the high-speed (infiniband) fabric, and the host code can communicate over this network for distributed/parallel applications.</p> <p>More details on FPGA can be found in the presentation below:</p> <p></p>"},{"location":"optimization/","title":"Optimizing SYCL programs for Intel\u00ae FPGA cards","text":"<p>Optimizing SYCL code for Intel FPGAs requires a combination of understanding the FPGA hardware, the SYCL programming model, and the specific compiler features provided by Intel. Here are some general guidelines to optimize Intel FPGA SYCL code.</p> <p>Compared to OpenCL, the Intel\u00ae oneAPI DPC++ compiler has enhanced features to detect possible optimizations( vectorization, static coalescing, etc ...). Nonetheless, some rules need to be followed to make sure the compiler is able to apply these optimizations. </p> <p>Optimizing your design</p> <p>As this course/workshop is only an introduction to the Intel\u00ae oneAPI for FPGA programming, we can't unfortunately provide all existing and possible optimizations. Many more optimizations can be found in the Intel official documentation.</p>"},{"location":"optimization/#loop-optimization","title":"Loop optimization","text":"<p>Loop unrolling is an optimization technique that aims to increase parallelism and, consequently, the throughput of certain computational tasks, particularly when implemented in hardware environments such as FPGAs. </p> <ol> <li> <p>Pipelining Synergy: Loop unrolling often goes hand in hand with pipelining in FPGAs. When loops are unrolled, each unrolled iteration can be pipelined, leading to even greater throughput enhancements.</p> </li> <li> <p>Resource Utilization: While loop unrolling can significantly speed up operations, it also consumes more FPGA resources, like Logic Elements (LEs) and registers, because of the duplicated hardware. Hence, there's a trade-off between speed and resource utilization.</p> </li> <li> <p>Memory Access: Unrolling loops that involve memory operations can lead to increased memory bandwidth utilization. In cases where memory bandwidth is a bottleneck, unrolling can provide substantial performance improvements.</p> </li> <li> <p>Latency &amp; Throughput: Loop unrolling doesn't necessarily reduce the latency of a single loop iteration (the time taken for one iteration to complete), but it can significantly improve the throughput (number of completed operations per unit time).</p> </li> <li> <p>Reduction in Control Logic: Unrolling can reduce the overhead associated with the loop control logic, such as incrementing the loop counter and checking the loop termination condition.</p> <p> Loop Optimization in HLS </p> </li> <li> <p>Unrolling loops will help to reduce the Initialization Interval (II) as you can notice on the previous figure.</p> </li> </ol> <p>Increasing throughput with loop unrolling</p> How to unroll loopsQuestionSolution <ul> <li>Unrolling loop can be done using the <code>#pragma unroll &lt;N&gt;</code></li> <li><code>&lt;N&gt;</code> is the unroll factor</li> <li><code>#pragma unroll 1</code> : prevent a loop in your kernel from unrolling</li> <li><code>#pragma unroll</code> : let the offline compiler decide how to unroll the loop  <pre><code>handler.single_task&lt;class example&gt;([=]() {\n    #pragma unroll\n        for (int i = 0; i &lt; 10; i++) {\n            acc_data[i] += i;\n        }\n    #pragma unroll 1\n    for (int k = 0; k &lt; N; k++) {\n        #pragma unroll 5\n        for (int j = 0; j &lt; N; j++) {\n            acc_data[j] = j + k;\n        }\n    }\n});\n</code></pre></li> </ul> <ul> <li>Consider the following code that you can find at <code>oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/Features/loop_unroll</code></li> <li>Note that Intel did not consider data alignment which could impact performance</li> <li>We included <code>#include &lt;boost/align/aligned_allocator.hpp&gt;</code> to create aligned std::vector</li> <li>The following SYCL code has been already compiled for you, execute it on the FPGA nodes for several data input size and record the throughput and kernel time</li> <li>What do you observe ? <pre><code>//==============================================================\n// Copyright Intel Corporation\n//\n// SPDX-License-Identifier: MIT\n// =============================================================\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\nusing namespace sycl;\n\nusing aligned64_vector= std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt;;\n\n// Forward declare the kernel name in the global scope.\n// This FPGA best practice reduces name mangling in the optimization reports.\ntemplate &lt;int unroll_factor&gt; class VAdd;\n\n// This function instantiates the vector add kernel, which contains\n// a loop that adds up the two summand arrays and stores the result\n// into sum. This loop will be unrolled by the specified unroll_factor.\ntemplate &lt;int unroll_factor&gt;\nvoid VecAdd(const aligned64_vector &amp;summands1,\n            const aligned64_vector &amp;summands2, aligned64_vector &amp;sum,\n            size_t array_size) {\n\n#if FPGA_SIMULATOR\n  auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n  auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n  auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n  try {\n    queue q(selector,property::queue::enable_profiling{});\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    buffer buffer_summands1(summands1);\n    buffer buffer_summands2(summands2);\n    buffer buffer_sum(sum);\n\n    event e = q.submit([&amp;](handler &amp;h) {\n      accessor acc_summands1(buffer_summands1, h, read_only);\n      accessor acc_summands2(buffer_summands2, h, read_only);\n      accessor acc_sum(buffer_sum, h, write_only, no_init);\n\n      h.single_task&lt;VAdd&lt;unroll_factor&gt;&gt;([=]()\n                                         [[intel::kernel_args_restrict]] {\n        // Unroll the loop fully or partially, depending on unroll_factor\n        #pragma unroll unroll_factor\n        for (size_t i = 0; i &lt; array_size; i++) {\n          acc_sum[i] = acc_summands1[i] + acc_summands2[i];\n        }\n      });\n    });\n\n    double start = e.get_profiling_info&lt;info::event_profiling::command_start&gt;();\n    double end = e.get_profiling_info&lt;info::event_profiling::command_end&gt;();\n    // convert from nanoseconds to ms\n    double kernel_time = (double)(end - start) * 1e-6;\n\n    std::cout &lt;&lt; \"unroll_factor \" &lt;&lt; unroll_factor\n              &lt;&lt; \" kernel time : \" &lt;&lt; kernel_time &lt;&lt; \" ms\\n\";\n    std::cout &lt;&lt; \"Throughput for kernel with unroll_factor \" &lt;&lt; unroll_factor\n              &lt;&lt; \": \";\n    std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(3)\n#if defined(FPGA_SIMULATOR)\n              &lt;&lt; ((double)array_size / kernel_time) / 1e3f &lt;&lt; \" MFlops\\n\";\n#else\n              &lt;&lt; ((double)array_size / kernel_time) / 1e6f &lt;&lt; \" GFlops\\n\";\n#endif\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n}\n\nint main(int argc, char *argv[]) {\n#if defined(FPGA_SIMULATOR)\n  size_t array_size = 1 &lt;&lt; 4;\n#else\n  size_t array_size = 1 &lt;&lt; 26;\n#endif\n\n  if (argc &gt; 1) {\n    std::string option(argv[1]);\n    if (option == \"-h\" || option == \"--help\") {\n      std::cout &lt;&lt; \"Usage: \\n&lt;executable&gt; &lt;data size&gt;\\n\\nFAILED\\n\";\n      return 1;\n    } else {\n      array_size = std::stoi(option);\n    }\n  }\n\n  aligned64_vector summands1(array_size);\n  aligned64_vector summands2(array_size);\n\n  aligned64_vector sum_unrollx1(array_size);\n  aligned64_vector sum_unrollx2(array_size);\n  aligned64_vector sum_unrollx4(array_size);\n  aligned64_vector sum_unrollx8(array_size);\n  aligned64_vector sum_unrollx16(array_size);\n\n  // Initialize the two summand arrays (arrays to be added to each other) to\n  // 1:N and N:1, so that the sum of all elements is N + 1\n  for (size_t i = 0; i &lt; array_size; i++) {\n    summands1[i] = static_cast&lt;float&gt;(i + 1);\n    summands2[i] = static_cast&lt;float&gt;(array_size - i);\n  }\n\n  std::cout &lt;&lt; \"Input Array Size:  \" &lt;&lt; array_size &lt;&lt; \"\\n\";\n\n  // Instantiate VecAdd kernel with different unroll factors: 1, 2, 4, 8, 16\n  // The VecAdd kernel contains a loop that adds up the two summand arrays.\n  // This loop will be unrolled by the specified unroll factor.\n  // The sum array is expected to be identical, regardless of the unroll factor.\n  VecAdd&lt;1&gt;(summands1, summands2, sum_unrollx1, array_size);\n  VecAdd&lt;2&gt;(summands1, summands2, sum_unrollx2, array_size);\n  VecAdd&lt;4&gt;(summands1, summands2, sum_unrollx4, array_size);\n  VecAdd&lt;8&gt;(summands1, summands2, sum_unrollx8, array_size);\n  VecAdd&lt;16&gt;(summands1, summands2, sum_unrollx16, array_size);\n\n  // Verify that the output data is the same for every unroll factor\n  for (size_t i = 0; i &lt; array_size; i++) {\n    if (sum_unrollx1[i] != summands1[i] + summands2[i] ||\n        sum_unrollx1[i] != sum_unrollx2[i] ||\n        sum_unrollx1[i] != sum_unrollx4[i] ||\n        sum_unrollx1[i] != sum_unrollx8[i] ||\n        sum_unrollx1[i] != sum_unrollx16[i]) {\n      std::cout &lt;&lt; \"FAILED: The results are incorrect\\n\";\n      return 1;\n    }\n  }\n  std::cout &lt;&lt; \"PASSED: The results are correct\\n\";\n  return 0;\n}\n</code></pre></li> </ul> Unroll factor kernel execution time (ms) Throughput (GFlops) 1 77 0.447 2 58 0.591 4 43 0.804 8 40 0.857 16 39 0.882 <ul> <li>Increasing the unroll factor improves throughput    </li> <li>Nonetheless, unrolling large loops should be avoided as it would require a large amount of hardware</li> </ul> <p>Recording kernel time</p> <ul> <li>In this example, we have also seen how to record kernel time.</li> <li>Using the property `property::queue::enable_profiling{}`` adds the requirement that the runtime must capture profiling information for the command groups that are submitted from the queue </li> <li>You can the capture  the start &amp; end time using the following two commands:<ul> <li><code>double start = e.get_profiling_info&lt;info::event_profiling::command_start&gt;();</code></li> <li><code>double end = e.get_profiling_info&lt;info::event_profiling::command_end&gt;();</code></li> </ul> </li> </ul> <p>Caution with nested loops</p> <ul> <li>Loop unrolling involves replicating the hardware of a loop body multiple times and reducing the trip count of a loop. Unroll loops to reduce or eliminate loop control overhead on the FPGA. </li> <li>Loop-unrolling can be used to eliminate nested-loop structures.</li> <li>However avoid unrolling the outer-loop which will lead to Resource Exhaustion and dramatically increase offline compilation</li> </ul>"},{"location":"optimization/#simd-work-items-for-nd-range-kernels","title":"SIMD Work Items for ND-Range kernels","text":"<ul> <li>ND-range kernel should use instead of classical data-parallel kernels</li> <li>The work-group size needs to be set using the attribute <code>[[sycl::reqd_work_group_size(1, 1, REQD_WG_SIZE)]]</code></li> <li>To specify the number of SIMD work_items, you will need to add the following attribute <code>[[intel::num_simd_work_items(NUM_SIMD_WORK_ITEMS)]]</code></li> <li>Note that NUM_SIMD_WORK_ITEMS should divide evenly REQD_WG_SIZE</li> <li>The supported values for NUM_SIMD_WORK_ITEMS  are 2, 4, 8, and 16</li> </ul> <p>Example</p> <pre><code>...\nh.parallel_for&lt;VectorAddID&gt;(\nsycl::nd_range&lt;1&gt;(sycl::range&lt;1&gt;(2048), sycl::range&lt;1&gt;(128)),        \n    [=](sycl::nd_item&lt;1&gt; it) \n    [[intel::num_simd_work_items(8),\n    sycl::reqd_work_group_size(1, 1, 128)]] {\n    auto gid = it.get_global_id(0);\n    accessor_c[gid] = accessor_a[gid] + accessor_b[gid];\n    });\n});\n...\n</code></pre> <ul> <li> <p>The 128 work-items are evenly distributed among 8 SIMD lanes</p> </li> <li> <p>128/8 = 16 wide vector operation</p> </li> <li> <p>The offline compiler coalesces 8 loads to optimize (reduce) the access to memory in case there are no data dependencies</p> </li> </ul>"},{"location":"optimization/#loop-coalescing","title":"Loop coalescing","text":"<p>Utilize the <code>loop_coalesce</code> attribute to instruct the Intel\u00ae oneAPI DPC++/C++ Compiler to merge nested loops into one, preserving the loop's original functionality. By coalescing loops, you can minimize the kernel's area consumption by guiding the compiler to lessen the overhead associated with loop management.</p> <p>Coalesced two loops</p> Using the loop_coalesce attribute <pre><code>[[intel::loop_coalesce(2)]]\nfor (int i = 0; i &lt; N; i++)\n   for (int j = 0; j &lt; M; j++)\n      sum[i][j] += i+j;\n</code></pre> Equivalent code <pre><code>int i = 0;\nint j = 0;\nwhile(i &lt; N){\n  sum[i][j] += i+j;\n  j++;\n  if (j == M){\n    j = 0;\n    i++;\n  }\n}\n</code></pre>"},{"location":"optimization/#memory","title":"Memory","text":""},{"location":"optimization/#static-coalescing","title":"Static coalescing","text":"<ul> <li> <p>Static coalescing is performed by the Intel\u00ae oneAPI DPC++/C++ Compiler contiguous accesses to global memory can be merged into a single wide access.</p> </li> <li> <p>For static memory coalescing to occur, your code should be structured so that the compiler can detect a linear access pattern at compile time. The initial kernel code depicted in the previous figure can leverage static memory coalescing, as all indices into buffers a and b increase with offsets recognizable during compilation.</p> </li> </ul> <p> </p> FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits -- Figure 17-21"},{"location":"optimization/#data-structure-alignment","title":"Data structure alignment","text":"<p>In order to performance, structure alignment can be modified to be properly aligned. By default, the offline compiler aligns these elements based on:</p> <ul> <li>The alignment should be a power of two.</li> <li>The alignment should be a multiple of the least common multiple (LCM) of the word-widths of the structure member sizes.</li> </ul> <p>Let's take a simple but clear example to understand why alignment is so important.</p> <p></p> <ul> <li> <p>Each element of MyStruct has 12 bytes  due to padding</p> </li> <li> <p>Recall that each transaction between the user kernel design and the memory controller is 512 bits wide to enable DMA</p> </li> <li> <p>We have therefore 64/12 = 5.333 =&gt; alignment is far from optimal as the 6<sup>th</sup> element of MyStruct will be split between two 64-byte regions</p> </li> </ul> <p></p> <ul> <li> <p>Removing all padding will definitely reduce the size </p> </li> <li> <p>Padding can be removed by adding  the \u201cpacked\u201d attribute, i.e, \u201cattribute((packed))\u201d in your kernel</p> </li> <li> <p>Each element of MyStruct will have therefore 9 bytes</p> </li> <li> <p>However, 64/9 = 7.111 =&gt; we still have some elements in multiple 64-bytes region and the alignment is sub-optimal</p> </li> </ul> <p></p> <ul> <li> <p>To improve performance, align structure  such all elements belongs to a single 64-byte regions</p> </li> <li> <p>Padding can still be removed by adding  the \u201cpacked\u201d attribute, i.e, \u201cattribute((packed))\u201d </p> </li> <li> <p>Transaction size is 64 bytes, the minimum alignment which is also a multiple of the transaction size is 16 </p> </li> <li> <p>Enforce a 16-byte alignment with <code>__attribute__((aligned(16)))</code></p> </li> </ul> <p></p> <p>Removing padding and changing structure alignment</p> CodeExecution time <ul> <li> <p>The following code show the impact of changing the alignmement and padding using three scenarii:</p> <ul> <li> <p>Default alignment and padding </p> </li> <li> <p>Removing padding</p> </li> <li> <p>Changing alignment </p> </li> </ul> </li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;typeinfo&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;chrono&gt;\nusing namespace std::chrono;\n\n#define ALIGNMENT 64\n#define IT 1024\n\n\nconstexpr int kVectSize = 2048;\n\n\ntemplate&lt;typename T&gt;\nvoid test_structure( T* device,sycl::queue &amp;q, int nb_iters){\n\n      sycl::event e;\n      const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n\n      auto start = high_resolution_clock::now();\n      sycl::buffer buffer_device{device, sycl::range(kVectSize),props};\n      e = q.submit([&amp;](sycl::handler &amp;h) {\n       sycl::accessor accessor_device{buffer_device, h, sycl::read_write};\n       h.single_task([=]() {\n       for(int it=0;it &lt; nb_iters ;it++){\n        for (int idx = 0; idx &lt; kVectSize; idx++) {\n          accessor_device[idx].C = (int)accessor_device[idx].A + accessor_device[idx].B;\n         }\n        }\n        });\n       });\n\n    sycl::host_accessor buffer_host(buffer_device);\n    auto stop = high_resolution_clock::now();\n    // convert from nanoseconds to ms\n    duration&lt;double&gt; kernel_time = stop - start;\n\n    std::cout  &lt;&lt; \" Time (\" &lt;&lt;typeid(T).name()&lt;&lt;  \") : \" &lt;&lt; kernel_time.count() &lt;&lt; \" ms\\n\";\n}\n\nint main() {\n\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector,sycl::property::queue::enable_profiling{});\n\n    auto device = q.get_device();\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    typedef struct {\n        char A;\n        int  B;\n        int  C;\n    } mystruct;\n\n    typedef struct __attribute__ ((packed)) {\n        char A;\n        int  B;\n        int  C;\n    } mystruct_packed;\n\n\n    typedef struct __attribute__ ((packed)) __attribute__ ((aligned(16))) {\n        char A;\n        int  B;\n        int  C;\n    } mystruct_packed_aligned;\n\n\n    mystruct* vec_a = new(std::align_val_t{ 64 }) mystruct[kVectSize];\n    mystruct_packed* vec_b = new(std::align_val_t{ 64 }) mystruct_packed[kVectSize];\n    mystruct_packed_aligned* vec_c = new(std::align_val_t{ 64 }) mystruct_packed_aligned[kVectSize];\n\n\n    for (int i = 0; i &lt; kVectSize; i++) {\n        vec_a[i].A = vec_b[i].A = vec_c[i].A = char(std::rand() % 256);\n        vec_a[i].B = vec_b[i].B = vec_c[i].B = std::rand();\n        vec_a[i].C = vec_b[i].C = vec_c[i].C = std::rand();\n    }\n\n    std::cout &lt;&lt; \"Packed with default alignment\" &lt;&lt; kVectSize &lt;&lt; std::endl;\n\n    test_structure&lt;mystruct&gt;(vec_a,q,IT);\n    test_structure&lt;mystruct_packed&gt;(vec_b,q,IT);\n    test_structure&lt;mystruct_packed_aligned&gt;(vec_c,q,IT);\n\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n\n    //sycl::free(vec_a,q);\n    //sycl::free(vec_b,q);\n    //sycl::free(vec_c,q);\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> Scenario Processing time (seconds) Default alignment and padding 14.33 Removing padding 6.35 Changing alignment 0.03"},{"location":"optimization/#memory_1","title":"Memory","text":"Type Access Hardware Host memory read/write only by host DRAM Global memory (device) read/write by host and work-items FPGA DRAM (HBM, DDR,QDR) Local memory (device) read/write only by work-group RAM blocks Constant memory (device) read/write by host read only  by work-items FPGA DRAM  RAM blocks Private memory device read/write by single work-item only RAM blocks Registers <ul> <li> <p>Transfers between host memory and global device memory should leverage DMA for efficiency.</p> </li> <li> <p>Of all memory types on FPGAs, accessing device global memory is the slowest.</p> </li> <li> <p>In practice, using local device memory is advisable to reduce global memory accesses.</p> </li> </ul>"},{"location":"optimization/#local-memory-in-nd-range-kernels","title":"Local memory in ND-Range kernels","text":"<ul> <li>You can improve memory access by using local and private memory.</li> <li>When you define a private array, group local memory, or a local accessor, the Intel\u00ae oneAPI DPC++/C++ Compiler generates kernel memory in the hardware. This kernel memory is often termed on-chip memory since it originates from memory resources, like RAM blocks, present on the FPGA.</li> <li>Local or private memory is a fast memory that should be favored when resources allow.</li> </ul> <p>Private memory</p> <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n// Create an accessor for device global memory from buffer buff\naccessor acc(buff, h, write_only);\ncgh.single_task([=]() {\n     // Declare a private array\n     int T[N];\n     // Write to private memory\n     for (int i = 0; i &lt; N; i++)\n        T[i] = i;\n     // Read from private memory and write to global memory through the accessor\n     for (int i = 0; i &lt; N; i+=2)\n        acc[i] = T[i] + T[i+1];\n     });\n}); \n...\n</code></pre> <ul> <li> <p>Two ways to define local memory for work-groups:</p> <ul> <li> <p>If you have function scope local data using group_local_memory_for_overwrite, the Intel\u00ae oneAPI DPC++/C++ Compiler statically sizes the local data that you define within a function body at compilation time.</p> </li> <li> <p>For accessors in the local space, the host assigns their memory sizes dynamically at runtime. However, the compiler must set these physical memory sizes at compilation time. By default, that size is 16 kB. </p> </li> </ul> </li> </ul> <p>Local memory</p> Using sycl::local_accessorUsing <code>group_local_memory</code> functions <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n    h.parallel_for(\n        nd_range&lt;1&gt;(range&lt;1&gt;(256), range&lt;1&gt;(16)), [=](nd_item&lt;1&gt; item)\n        [[intel::max_work_group_size(1, 1, 16)]] { \n        int local_id = item.get_local_id();\n        sycl::local_accessor&lt;float,2&gt; lmem{B, h};\n        lmem[local_id] = local_id++;\n        });\n    });\n... \n</code></pre> <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n    h.parallel_for(\n        nd_range&lt;1&gt;(range&lt;1&gt;(256), range&lt;1&gt;(16)), [=](nd_item&lt;1&gt; item) {\n        int local_id = item.get_local_id();\n        auto ptr = group_local_memory_for_overwrite&lt;int[16]&gt;(item.get_group());\n        auto&amp; ref = *ptr;\n        ref[local_id] = local_id++ ;\n        });\n    });\n... \n</code></pre> <ul> <li>The ND-Range kernel has 16 workgroups with 16 work items for each group.</li> <li>A group-local variable (int[16]) is created for each group and shared through a multi_ptr to all work-items of the same group</li> </ul> <ul> <li>Use sycl::local_accessor when data needs to be retained and shared within the same work-group and when precise control over data access and synchronization is necessary. Opt for group_local_memory_for_overwrite when you need temporary storage that can be efficiently reused across multiple work-groups, without persistence or heavy synchronization overhead.</li> </ul>"},{"location":"optimization/#kernel-memory-system","title":"Kernel Memory System","text":"<ul> <li> <p>Before diving into the different settings, we need to introduce some definitions:</p> <ul> <li> <p>Port: a memory port serves as a physical access point to memory, connecting to one or more load-store units (LSUs) within the datapath. An LSU can interface with multiple ports, and a port can be linked to multiple LSUs.</p> </li> <li> <p>Bank: a memory bank is a division within the kernel memory system, holding a unique subset of the kernel's data. All data is distributed across these banks, and every memory system has at least one bank.</p> </li> <li> <p>Replicate: a memory bank replicate is a copy of the data within a memory bank, with each replicate containing identical data. Replicates are independently accessible, and every memory bank includes at least one replicate.</p> </li> <li> <p>Private Copy: a private copy is a version of data within a replicate, created for nested loops to support concurrent outer loop iterations. Each outer loop iteration has its own private copy, allowing different data per iteration.</p> </li> </ul> </li> </ul>"},{"location":"optimization/#settings-memory-banks","title":"Settings memory banks","text":"<ul> <li>Local data can be stored  in separate  local memory banks for parallel memory accesses</li> <li>Number of banks of a local memory can be adjusted (e.g., to increase the parallel access) </li> <li>Add the following attributes <code>[[intel::numbanks(#NB), intel::bankwidth(#BW)]]</code>:  <ul> <li><code>#NB</code> : number of banks </li> <li><code>#BW</code> : bankwidth to be considered in bytes</li> </ul> </li> <li>Ex: <code>[[intel::numbanks(8), intel::bankwidth(16)]] int lmem[8][4]</code>; </li> <li>All rows accessible in parallel with numbanks(8) </li> <li>Different configurations patterns can be adopted </li> </ul>  [[intel::numbanks(8), intel::bankwidth(16)]] lmem[8][4]  (source: Intel)   <p>Masking the last index</p> <ul> <li>Intel's documentation states that \"To enable parallel access, you must mask the dynamic access on the lower array index\" <pre><code>[[intel::numbanks(8), intel::bankwidth(16)]] int lmem[8][4];\n#pragma unroll\nfor (int i = 0; i &lt; 4; i+=2) {\n    lmem[i][x &amp; 0x3] = ...;\n} \n</code></pre></li> </ul>"},{"location":"optimization/#local-memory-replication","title":"Local memory replication","text":"<p>Example</p> <p> <pre><code>[[intel::fpga_memory,\nintel::singlepump,\nintel::max_replicates(3)]] int lmem[16]; \nlmem[waddr] = lmem[raddr] +\n              lmem[raddr + 1] +\n              lmem[raddr + 2]; \n</code></pre> <ul> <li>The offline compiler can replicate the local memory</li> <li>This allows to create multiple ports </li> <li>Behaviour: <ul> <li>All read ports will be accessed in parallel </li> <li>All write ports are connected together</li> <li>Data between replicate is identical </li> </ul> </li> <li>Parallel access to all ports is possible but consumes more hardware resources</li> <li><code>[[intel::max_replicates(N)]]</code> control the replication factor</li> </ul> <p> </p>"},{"location":"reporting_profiling/","title":"Reporting &amp; Profiling SYCL programs for Intel\u00ae FPGA cards","text":"<p>After having spent some time to write your kernel and debug functional problems, it's now time to take advantage of the accelerator. FPGA uses pipelining parallelism but how does it work ? </p> <p></p>"},{"location":"reporting_profiling/#definitions","title":"Definitions","text":"<p>Pipelining (see FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits)</p> <p>Pipelining is a design technique used in synchronous digital circuits to increase fMAX. Pipelining involves adding registers to the critical path, which decreases the amount of logic between each register. Less logic takes less time to execute, which enables an increase in fMAX. The critical path in a circuit is the path between any two consecutive registers with the highest latency. That is, the path between two consecutive registers where the operations take the longest to complete. Pipelining is especially useful when processing a stream of data. A pipelined circuit can have different stages of the pipeline operating on different input stream data in the same clock cycle, which leads to better data processing throughput. </p> <p>Maximum Frequency (fMAX)</p> <p>The fMAX of a digital circuit is its highest possible clock frequency, determining the maximum rate for updating register outputs. This speed is constrained by the physical propagation delay of the signal across the combinational logic between consecutive register stages. The delay is affected by the complexity of the combinational logic in the path, and the path with the greatest number of logic elements and highest delay sets the speed limit for the entire circuit, often known as the critical path. The fMAX is the reciprocal of this critical path delay, and having a high fMAX is desirable as it leads to better performance when there are no other restrictions. </p> <p>Throughput</p> <p>Throughput in a digital circuit refers to the speed at which data is handled. When there are no other limiting factors, a higher fMAX leads to increased throughput, such as more samples per second. Often synonymous with performance, throughput is frequently used to gauge the effectiveness of a circuit. </p> <p>Latency</p> <p>Latency measures the duration to complete operations in a digital circuit, and it can be assessed for individual tasks or the whole circuit. It can be measured in time units like microseconds or clock cycles, with the latter often preferred. Measuring latency in clock cycles separates it from the circuit's clock frequency, making it easier to understand the real effects of modifications on the circuit's performance. </p> <p>Occupancy</p> <p>The occupancy of a datapath at a specific moment signifies the fraction of the datapath filled with valid data. When looking at a circuit's execution of a program, the occupancy is the mean value from the beginning to the end of the program's run. Parts of the datapath that are unoccupied are commonly called bubbles, akin to a CPU's no-operation (no-ops) instructions, which don't influence the final output. Minimizing these bubbles leads to greater occupancy. If there are no other hindrances, optimizing the occupancy of the datapath will boost throughput.  Occupancy: \u2156=40% </p>"},{"location":"reporting_profiling/#reporting","title":"Reporting","text":"<p>If you instruct the DPC++ compiler to stop compiling the design after generating the early image, you will be able to access precious information including performance and area estimates without having to wait many hours. The FPGA Early Image can be analyzed using the FPGA Optimization Report to provide:</p> <ul> <li>loop analysis</li> <li>area estimates</li> <li>kernel memory information </li> <li> <p>scheduler information</p> </li> <li> <p>Recall that the FPGA Early image can be obtained using the command: <code>icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xsboard=p520_hpc_m210h_g3x16 accumulator.cpp -o accumulator_report.a</code></p> </li> <li> <p>You can evaluate whether the estimated kernel performance data is satisfactory by going to the /reports/ directory and examining one of the following files related to your application: <li> <p><code>report.html</code>: This file can be viewed using Internet browsers of your choice</p> </li> <li><code>&lt;design_name&gt;.zip</code>: Utilize the Intel\u00ae oneAPI FPGA Reports tool,i.e., <code>fpga_report</code></li> <p>Analyzing the FPGA Early Image report</p> accumulator.cppSetupQuestionSolution <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass Accumulator;\n\nconstexpr int kVectSize = 256;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    double * vec = new(std::align_val_t{ 64 }) double[kVectSize];\n    double res = 0;\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec[i] = 1.0;\n    }\n\n    std::cout &lt;&lt; \"Accumulate values \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_in{vec, sycl::range(kVectSize)};\n      sycl::buffer buffer_out{&amp;res, sycl::range(1)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor arr{buffer_in, h, sycl::read_only};\n        sycl::accessor result{buffer_out, h, sycl::write_only,sycl::no_init};\n\n        h.single_task&lt;Accumulator&gt;([=]() {\n      double temp_sum = 0;\n          for (int i = 0; i &lt; kVectSize; ++i)\n            temp_sum += arr[i];\n          result[0] = temp_sum;\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that Accumulation is correct\n    double expected = 0.0; \n    for (int i = 0; i &lt; kVectSize; i++) \n      expected += vec[i];\n\n    if (res != expected) {\n        std::cout &lt;&lt; \"res = \" &lt;&lt; res &lt;&lt;  \", expected = \"\n                  &lt;&lt; expected &lt;&lt; std::endl;\n        passed = false;\n      }\n\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>First,copy the <code>accumulator.cpp</code> to  your home folder    </li> <li>Generate the early image with the report using:  <pre><code>  # Don't forget to be on a node first\n  icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xsboard=p520_hpc_m210h_g3x16 accumulator.cpp -o accumulator_report.a`\n</code></pre></li> <li>The next step is to download the HTML report located inside the <code>accumulator_report.prj/reports</code> directory on local machine</li> <li>Open the file <code>report.html</code> with a web browser</li> </ul> <p></p> <ul> <li>Check the loop analysis report. What do you observe ?</li> <li>What is the predicted fMAX ?</li> <li>What is the block scheduled II ? What is its impact ? </li> </ul> <ul> <li>We have a data-dependency at line 59</li> <li>For each loop iteration, the Intel\u00ae oneAPI DPC++/C++ Compiler takes 3 cycles to compute the result of the addition and then stores it in the variable temp_sum</li> <li>We either need to remove the data dependency or to relax it</li> </ul>"},{"location":"reporting_profiling/#profiling","title":"Profiling","text":"<ul> <li>The Intel\u00ae FPGA dynamic profiler for DPC++ can be used to add performance counters to the design and collect performance data during execution. This requires full hardware compilation.</li> <li>In order to instruct the offline compiler to add those performance counters, one needs to add the compilation option <code>-Xsprofile</code>.</li> </ul> Intel\u00ae FPGA Dynamic Profiler for DPC++: Performance Counters Instrumentation <p>Performance counters are attached to every load and store instruction, and they are linked together in a sequence that connects to the Control Register Access (CRA). The CRA interface provides access to the control and status registers interface.</p> <ul> <li>Once the design has been compiled, performance data can be obtained at runtime:<ul> <li>Either using your host application in the Intel\u00ae VTuneTM Profiler with CPU/FPGA Interaction view</li> <li>Or using the command line by using the Profiler Runtime Wrapper. Data can later be importer to the  Intel\u00ae VTuneTM Profiler</li> </ul> </li> </ul>"},{"location":"reporting_profiling/#profiler-runtime-wrapper","title":"Profiler Runtime Wrapper","text":"<ul> <li>You need to execute the FPGA executable using the Profiler Runtime Wrapper to fill the profiling results using the following command: <pre><code>aocl profile [options] /path/to/executable [executable options]\n</code></pre></li> <li>The Profiler Runtime Wrapper calls your executable and collects profile information</li> <li>The performance counter data is saved in a <code>profile.mon</code> monitor description file that the Profiler Runtime Wrapper post-processes and outputs into a readable <code>profile.json</code> file</li> <li> <p>Intel recommends the use of the <code>profile.json</code> for further data processing</p> </li> <li> <p>Note that you can control the sample rate used by the Profiler Runtime Wrapper using the <code>-period &lt;N&gt;</code> option</p> </li> <li> <p>Use the command <code>aocl profile -help</code> to get more details: <pre><code> aocl profile --help\n   aocl profile can be used to collect information about your host run if you compiled with -profile. \n   To use it, please specify your host executable as follows: 'aocl profile path/to/executable'. \n\n   If you are compiling with the oneAPI Data Parallel C++ Compiler and do not wish to pass in the \n   compiled executable binary directly (but rather a script that calls it), the binary needs to be \n   passed in using '--executable' or '-e'. \n\n   It is also optional (but recommended) that you include the location of the *.aocx file \n   using '--aocx' or '-x'. Note that this file is not needed when compiling with the \n   oneAPI Data Parallel C++ Compiler. If no files are given, any aocx found \n   in the current directory will be used (potentially incorrectly) \n\n   OpenCL use case example: aocl profile -x kernel.aocx bin/host -host-arg1 -host-arg2 \n\n   oneAPI use case example: aocl profile -e kernel.fpga executable_calling_kernel_fpga.sh -executable-arg1 \n\n   You can also specify a few other options (after the executable if relevant): \n     - Adjust the period between counter readbacks by specifying the number of clock cycles between subsequent\n       readbacks with '-period ###': 'aocl profile path/to/executable -period 10000' \n       If this flag is not passed in, the counters will be read back as often as possible. \n     - Change counters to only read when the kernel finishes (not while it's running) with -no-temporal \n     - Turn off memory transfer information with -no-mem-transfers \n     - Turn on shared counter data (use when design compiled with '-profile-shared-counters' option) \n     - Change the output directory where the .mon and .json file will be placed with '-output-dir /path/to/new/loc/' \n     - Skip the actual compile and just point to a profile.mon file with '-no-run /path/to/profile.mon' \n       Do this if you already have data, but want it in a format that VTune can display. \n     - Do not create a profile.json file by setting the '-no-json' flag (no need for .aocx or .source files) \n       Do this if you do not wish to visualize the profiler data with VTune, and want the profile.mon output\n\n   Please ensure that the executable and its options are the last arguments. \n</code></pre></p> </li> </ul>"},{"location":"reporting_profiling/#performance-data","title":"Performance data","text":"Stall (%)Occupancy (%)Bandwidth <p>The percentage of time that memory or pipe access leads to pipeline stalls represents a measure of the efficiency of the memory or pipe in fulfilling access requests. It reflects how often these components can successfully meet a request without causing a delay or interruption in the processing flow.</p> <p>the proportion of the total time profiled in which a valid work-item is executing a memory or pipe instruction. It quantifies the fraction of the observed period when actual processing tasks related to memory or pipe instructions are being carried out.</p> <p>The average memory bandwidth refers to the efficiency of memory access in an FPGA. For each global memory access, resources are assigned to obtain data, but the kernel program might use less than the acquired amount. The overall efficiency is the percentage of the total bytes retrieved from the global memory system that the kernel program actually utilizes.</p>"},{"location":"reporting_profiling/#example","title":"Example","text":"<p>Improve Bandwith using vectorization</p> vector_add_ndrange.cppProfilingVectorization <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nconstexpr int kVectSize = 2048;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\n    int * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\n    int * vec_c = new(std::align_val_t{ 64 }) int[kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec_a[i] = i;\n      vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\n      sycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\n      sycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        h.parallel_for&lt;VectorAddID&gt;(sycl::range(kVectSize),[=](sycl::id&lt;1&gt; idx) {\n      accessor_c[idx] = accessor_a[idx] + accessor_b[idx];\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = vec_a[i] + vec_b[i];\n      if (vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li> <p>First,copy the <code>vector_add_ndrange.cpp</code> to your home folder    </p> </li> <li> <p>The kernel use a simple data-parallel kernel (no work-groups) to sum two arrays (size=2048) </p> </li> <li> <p>Perform a full hardware compilation with <code>-Xsprofile</code>flag: <code>icpx -fsycl -fintelfpga -qactypes -Xshardware -Xsprofile -Xsboard=p520_hpc_m210h_g3x16 -DFPGA_HARDWARE vector_add_ndrange.cpp -o vector_add_ndrange.fpga</code></p> </li> <li> <p>Execute the FPGA executable using the Profiler Runtime Wrapper: <code>aocl profile -e ./vector_add_ndrange.fpga</code></p> </li> <li> <p>Create a folder <code>profiling</code> and place the new <code>profile.json</code> file inside the folder</p> </li> <li> <p>Follow this guide to open the VTune GUI interface</p> </li> </ul> <p></p> <ul> <li>Create a new project and import the <code>profiling</code> folder </li> </ul> <p></p> <ul> <li>Once loaded, open the \"Bottom-up\" tab. You should see something similar as the figure below.</li> </ul> <p></p> <ul> <li>We have a high occupancy and a average bandwidth of 4.5 GB/s far from the theoretical bandwidth of 12.8 GB/s for a single pseudo-channel </li> </ul> <ul> <li> <p>You will first need to use a ND-range kernel and define your work-group size using the attribute <code>[[sycl::reqd_work_group_size(1, 1, REQD_WG_SIZE)]]</code></p> </li> <li> <p>To specify the number of SIMD work_items, you will need to add the following attribute <code>[[intel::num_simd_work_items(NUM_SIMD_WORK_ITEMS)]]</code> with <code>NUM_SIMD_WORK_ITEMS</code> dividing evenly <code>REQD_WG_SIZE</code></p> </li> <li> <p>The supported values for <code>NUM_SIMD_WORK_ITEMS</code> are 2, 4, 8, and 16</p> </li> <li> <p>Example <pre><code>...\nh.parallel_for&lt;VectorAddID&gt;(\nsycl::nd_range&lt;1&gt;(sycl::range&lt;1&gt;(2048), sycl::range&lt;1&gt;(128)),        \n    [=](sycl::nd_item&lt;1&gt; it) \n    [[intel::num_simd_work_items(8),\n    sycl::reqd_work_group_size(1, 1, 128)]] {\n    auto gid = it.get_global_id(0);\n    accessor_c[gid] = accessor_a[gid] + accessor_b[gid];\n    });\n});\n...\n</code></pre></p> </li> <li> <p>The 128 work-items are evenly distributed among 8 SIMD lanes</p> </li> <li> <p>128/8 = 16 wide vector operation</p> </li> <li> <p>The offline compiler coalesces 8 loads to optimize (reduce) the access to memory in case there are no data dependencies</p> </li> </ul> <p></p> <ul> <li>The bandwidth is now 9.6 GB/s</li> </ul>"},{"location":"writing/","title":"Developing SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"writing/#anatomy-of-a-sycl-program","title":"Anatomy of a SYCL program","text":""},{"location":"writing/#data-management","title":"Data Management","text":"<p>In the context of SYCL, Unified Shared Memory (USM) and buffers represent two different ways to handle memory and data management. They offer different levels of abstraction and ease of use, and the choice between them may depend on the specific needs of an application. Here's a breakdown of the differences:</p>"},{"location":"writing/#unified-shared-memory-usm","title":"Unified Shared Memory (USM)","text":"<p>Unified Shared Memory is a feature that simplifies memory management by providing a shared memory space across the host and various devices, like CPUs, GPUs, and FPGAs. USM provides three different types of allocations:</p> <ol> <li>Device Allocations: Allocated memory is accessible only by the device.</li> <li>Host Allocations: Allocated memory is accessible by the host and can be accessed by devices. However, the allocated memory is stored on the host global memory. </li> <li>Shared Allocations: Allocated memory is accessible by both the host and devices. The allocated memory is present in both global memories and it is synchronized between host and device.</li> </ol> <p>USM allows for more straightforward coding, akin to standard C++ memory management, and may lead to code that is easier to write and maintain. </p> <p>FPGA support</p> <p>SYCL USM host allocations are only supported by some BSPs, such as the Intel\u00ae FPGA Programmable Acceleration Card (PAC) D5005 (previously known as Intel\u00ae FPGA Programmable Acceleration Card (PAC) with Intel\u00ae Stratix\u00ae 10 SX FPGA).</p> <p>Using SYCL, you can verify if you have access to the different features:</p> <p>Verify USM capabilities</p> <pre><code>if (!device.has(sycl::aspect::usm_shared_allocations)) {\n    # Try to default to host allocation only\n    if (!device.has(sycl::aspect::usm_host_allocations)) {\n        # Default to device and explicit data movement\n        std::array&lt;int,N&gt; host_array;\n        int *my_array = malloc_device&lt;int&gt;(N, Q);\n    }else{\n        # Ok my_array is located on host memory but transferred to device as needed\n        int* my_array = malloc_host&lt;int&gt;(N, Q);\n    }\n}else{\n        # Ok my_array is located on both global memories and synchronized automatically \n        int* shared_array = malloc_shared&lt;int&gt;(N, Q);\n}\n</code></pre> <p>That's not all</p> <ul> <li>Concurrent accesses and atomic modifications are not necessarily available even if you have host and shared capabilities.</li> <li>You need to verify <code>aspect::usm_atomic_shared_allocations</code> and <code>aspect::usm_atomic_host_allocations</code>.</li> </ul> <p>Bittware 520N-MX</p> <p>The USM host allocations is not supported by some BSPs. We will therefore use explicit data movement.</p> <p>Explicit USM</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Replace the original code with explicit USM code </li> <li>Verify your code using emulation</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\n               int len) {\n  for (int idx = 0; idx &lt; len; idx++) {\n    int a_val = vec_a_in[idx];\n    int b_val = vec_b_in[idx];\n    int sum = a_val + b_val;\n    vec_c_out[idx] = sum;\n  }\n}\n\nconstexpr int kVectSize = 256;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int host_vec_a[kVectSize];\n    int host_vec_b[kVectSize];\n    int host_vec_c[kVectSize];\n    int * vec_a = malloc_device&lt;int&gt;(kVectSize,q);\n    int * vec_b = malloc_device&lt;int&gt;(kVectSize,q);\n    int * vec_c = malloc_device&lt;int&gt;(kVectSize,q);\n    for (int i = 0; i &lt; kVectSize; i++) {\n      host_vec_a[i] = i;\n      host_vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n\n    q.memcpy(vec_a, host_vec_a, kVectSize * sizeof(int)).wait();\n    q.memcpy(vec_b, host_vec_b, kVectSize * sizeof(int)).wait();\n\n\n\n    q.single_task&lt;VectorAddID&gt;([=]() {\n        VectorAdd(vec_a, vec_b, vec_c, kVectSize);\n      }).wait();\n\n    q.memcpy(host_vec_c, vec_c, kVectSize * sizeof(int)).wait();\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = host_vec_a[i] + host_vec_b[i];\n      if (host_vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; host_vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; host_vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; host_vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    sycl::free(vec_a,q);\n    sycl::free(vec_b,q);\n    sycl::free(vec_c,q);\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre>"},{"location":"writing/#buffer-accessors","title":"Buffer &amp; accessors","text":"<p>Buffers and accessors are key abstractions that enable memory management and data access across various types of devices like CPUs, GPUs, DSPs, etc.</p> <ol> <li> <p>Buffers:Buffers in SYCL are objects that represent a region of memory accessible by the runtime. They act as containers for data and provide a way to abstract the memory management across host and device memories. This allows for efficient data movement and optimization by the runtime, as it can manage the data movement between host and device memory transparently.</p> </li> <li> <p>Accessors:Accessors provide a way to access the data inside buffers. They define the type of access (read, write, read-write) and can be used within kernels to read from or write to buffers.</p> </li> </ol> <p>Advantage</p> <p>Through the utilization of these accessors, the SYCL runtime examines the interactions with the buffers and constructs a dependency graph that maps the relationship between host and device functions. This enables the runtime to automatically orchestrate the transfer of data and the sequencing of kernel activities.</p> <p>Using Buffers and Accessors</p> <pre><code>    #include &lt;array&gt; \n    // oneAPI headers\n    #include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n    #include &lt;sycl/sycl.hpp&gt;\n\n    class Kernel;\n    constexpr int N = 100;\n    std::array&lt;int,N&gt; in_array;\n    std::array&lt;int,N&gt; out_array;\n    for (int i = 0 ; i &lt;N; i++)\n        in_array[i] = i+1;\n    queue device_queue(sycl::ext::intel::fpga_selector_v);\n\n    { // This one is very important to define the buffer scope\n      // buffer&lt;int, 1&gt; in_device_buf(in.data(), in.size());\n      // Or more convenient\n\n      buffer in_device_buf(in_array);\n      buffer out_device_buf(out_array);\n      device_queue.submit([&amp;](handler &amp;h) {\n        accessor in(in_device_buf, h, read_only);\n        accessor out(out_device_buf, h, write_only, no_init);\n        h.single_task&lt;Kernel&gt;([=]() { });\n      };\n    } \n    // Accessor going out of the scope\n    // Data has been copied back !!!\n</code></pre> <p>What about memory accesses in FPGA ? </p> <ul> <li>For FPGAs, the access pattern, access width, and coalescing of memory accesses can significantly affect performance. You might want to make use of various attributes and pragmas specific to your compiler and FPGA to guide the compiler in optimizing memory accesses.</li> <li>In order to use Direct Memory Access (DMA), you will need to setup proper data alignment or the offline compiler will output the following warnings: <pre><code>Running on device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nadd two vectors of size 256\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb60b350) and/or dev offset (0x400) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb611910) and/or dev offset (0x800) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from device to host because of lack of alignment\n**                 host ptr (0xb611d20) and/or dev offset (0xc00) is not aligned to 64 bytes\n</code></pre></li> <li>For example, you may need to replace: <pre><code>    int * vec_a = new int[kVectSize];\n    int * vec_b = new int[kVectSize];\n    int * vec_c = new int[kVectSize];\n</code></pre> by these ones: <pre><code>   int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\n   int * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\n   int * vec_c = new(std::align_val_t{ 64 }) int[kVectSize]; \n</code></pre></li> <li>OR We strongly recommend you to load our <code>jemalloc</code> module which provides such default alignment:   <pre><code>module load jemalloc\nexport JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./exe\n</code></pre></li> </ul>"},{"location":"writing/#queue","title":"Queue","text":"<p>Contrary to OpenCL, queues in SYCL are out-of-order by default. Nonetheless, you can change this behavior you declare it in your code.</p> <p>In-order-queue</p> <p> <pre><code>  ... \n  queue device_queue{sycl::ext::intel::fpga_selector_v,{property::queue::in_order()}};\n  // Task A\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskA&gt;([=]() { });\n  });\n  // Task B\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskB&gt;([=]() { });\n  });\n  // Task C\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskC&gt;([=]() { });\n  }); \n  ...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nB[TaskB] --&gt; C[TaskC];</code></pre> </p> <p>This behavior is not very useful nor flexible. Queue objects, by default, are out-of-order queues, except when they're constructed with the in-order queue property. Because of this, they must include mechanisms to arrange tasks that are sent to them. The way queues organize tasks is by allowing the user to notify the runtime about the dependencies that exist between these tasks. These dependencies can be described in two ways: either explicitly or implicitly, through the use of command groups.</p> <p>A command group is a specific object that outlines a task and its dependencies. These groups are generally expressed as C++ lambdas and are handed over as arguments to the submit() method within a queue object. The single parameter within this lambda is a reference to a handler object, utilized inside the command group to define actions, generate accessors, and outline dependencies.</p>"},{"location":"writing/#explicit-dependencies","title":"Explicit dependencies","text":"<p>Like for OpenCL, you can manage dependencies explicitly using events. </p> <p>Using events</p> <p> <pre><code>  ... \n  queue device_queue{sycl::ext::intel::fpga_selector_v};\n  // Task A\n  auto event_A = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskA&gt;([=]() { });\n  });\n  event_A.wait();\n  // Task B\n  auto event_B = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskB&gt;([=]() { });\n  });\n  // Task C\n  auto event_C = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskC&gt;([=]() { });\n  });\n  // Task D\n  device_queue.submit([&amp;](handler &amp;h) {\n  h.depends_on({event_B, event_C});\n  h.parallel_for(N, [=](id&lt;1&gt; i) { /*...*/ });\n  }).wait();\n  ...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nA[TaskA] --&gt; C[TaskC];\nB[TaskB] --&gt; D[TaskD];\nC[TaskC] --&gt; D[TaskD];</code></pre> </p> <ul> <li>Explicit dependencies using events is relevant when you use USM since buffers make use of accessors to model data dependencies.</li> <li>They are three possibilities to declare a dependcies explicitely:</li> <li>Calling the method <code>wait()</code> on the queue it-self</li> <li>Calling the method <code>wait</code> on the event return by the queue after submitting a command</li> <li>Calling the method <code>depends_on</code> of the handler object</li> </ul>"},{"location":"writing/#implicit-dependencies","title":"Implicit dependencies","text":"<ul> <li>Implicit dependencies occurs when your are using buffer &amp; accessor.</li> <li> <p>Accessors have different access modes:</p> </li> <li> <p>read_only: The content of the buffer can only be accessed for reading. So the content will only be copied once to the device</p> </li> <li>write_only: The content of the buffer can only be accessed for writing. The content of buffer is still copied from host to device before the kernel starts </li> <li>read_write: The content of the buffer can be accessed for reading and writing.</li> </ul> <p>You can add the <code>no_init</code> property to an accessor in <code>write_only</code> mode. This tells the runtime that the original data contains in the buffer can be ignored and don't need to be copied from host to device.</p> <p>Implicit dependencies obey to three main patterns (see DPC++ book):</p> <ul> <li>Read-after-Write  (RAW) : occurs when some data modified by a kernel should be read by another kernel. </li> <li>Write-after-Read  (WAR) : occurs when some data read by a kernel will be modified by another one</li> <li>Write-after-Write (WAW) : occurs when two kernels modified the same data</li> </ul> <p>Implicit dependencies</p> QuestionSolution <ul> <li>By default without access mode, each accessor will be read_write inducing unnecessary copies.</li> <li>Note also the first use of <code>host_accessor</code>. Why did we use it here ?</li> <li>Modifiy the following code to take into account implicit dependencies.  <pre><code>   constexpr int N = 100;\n   queue Q;\n   buffer&lt;int&gt; A{range{N}};\n   buffer&lt;int&gt; B{range{N}};\n   buffer&lt;int&gt; C{range{N}};\n   Q.submit([&amp;](handler &amp;h) {\n      accessor aA{A, h};\n      accessor aB{B, h};\n      accessor aC{C, h};\n      h.single_task&lt;Kernel1&gt;([=]() { \n         for(unsigned int i =0; i&lt;N; i++)\n             aA[i] = 10;\n             aB[i] = 50;\n             aC[i] = 0;\n      });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aA{A, h};\n       accessor aB{B, h};\n       accessor aC{C, h};\n       h.single_task&lt;Kernel2&gt;([=]() { \n          for(unsigned int i =0; i&lt;N; i++)\n             aC[i] += aA[i] + aB[i]; \n        });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aC{C, h};\n       h.single_task&lt;Kernel3&gt;([=]() {\n         for(unsigned int i =0; i&lt;N; i++)\n            aC[i]++; \n       });\n   });\n   host_accessor result{C};\n</code></pre></li> </ul> <p><pre><code>   constexpr int N = 100;\n   queue Q;\n   buffer&lt;int&gt; A{range{N}};\n   buffer&lt;int&gt; B{range{N}};\n   buffer&lt;int&gt; C{range{N}};\n   Q.submit([&amp;](handler &amp;h) {\n      accessor aA{A, h, write_only, no_init};\n      accessor aB{B, h, write_only, no_init};\n      accessor aC{C, h, write_only, no_init};\n      h.single_task&lt;Kernel1&gt;([=]() { \n         for(unsigned int i =0; i&lt;N; i++)\n             aA[i] = 10;\n             aB[i] = 50;\n             aC[i] = 0;\n      });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aA{A, h, read_only};\n       accessor aB{B, h, read_only};\n       accessor aC{C, h, write_only};\n       h.single_task&lt;Kernel2&gt;([=]() { \n          for(unsigned int i =0; i&lt;N; i++)\n             aC[i] += aA[i] + aB[i]; \n        });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aC{C, h, write_only};\n       h.single_task&lt;Kernel3&gt;([=]() {\n         for(unsigned int i =0; i&lt;N; i++)\n            aC[i]++; \n       });\n   });\n   host_accessor result{C, read_only};\n</code></pre> * The <code>host_accessor</code> obtains access to buffer on the host and will wait for device kernel to execute to generate data.</p>"},{"location":"writing/#parallelism-model-for-fpga","title":"Parallelism model for FPGA","text":"<p>Vectorization</p> <p>Vectorization is not the main source of parallelism but help designing efficient pipeline. Since hardware can be reconfigured at will. The offline compiler can design N-bits Adders, multipliers which simplify greatly vectorization. In fact, the offline compiler vectorizes your design automatically if possible.</p>"},{"location":"writing/#pipelining-with-nd-range-kernels","title":"Pipelining with ND-range kernels","text":"<ul> <li>ND-range kernels are based on a hierachical grouping of work-items</li> <li>A work-item represents a single unit of work </li> <li>Independent simple units of work don't communicate or share data very often</li> <li>Useful when porting a GPU kernel to FPGA</li> </ul> DPC++ book -- Figure 17-15  <ul> <li>FPGAs are different from GPU (lots of threads started at the same time)</li> <li>Impossible to replicate a hardware for a million of work-items</li> <li>Work-items are injected into the pipeline</li> <li>A deep pipeline means lots of work-items executing different tasks in parallel</li> </ul> DPC++ book -- Figure 17-16  <ul> <li>In order to write basic data-parallel kernel, you will need to use the <code>parallel_for</code> method. Below is an example of simple data-parallel kernel. As you can notice it, there is no notion of groups nor sub-groups. </li> </ul> <p>Matrix addition</p> <pre><code>   constexpr int N = 2048;\n   constexpr int M = 1024;\n   queue.submit([&amp;](sycl::handler &amp;h) {\n     sycl::accessor acc_a{buffer_a, h, sycl::read_only};\n     sycl::accessor acc_b{buffer_b, h, sycl::read_only};\n     sycl::accessor acc_c{buffer_c, h, sycl::read_write, sycl::no_init};\n     h.parallel_for(range{N, M}, [=](sycl::id&lt;2&gt; idx) {\n      acc_c[idx] = acc_a[idx] + acc_b[idx];\n     });\n   });\n</code></pre> <p>Vector addition</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Adapt the <code>vector_add.cpp</code> single-task kernel to a basis data-parallel kernel</li> <li>Emulate to verify your design</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nconstexpr int kVectSize = 256;\n\nint main() {\nbool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\n    int * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\n    int * vec_c = new(std::align_val_t{ 64 }) int[kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec_a[i] = i;\n      vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\n      sycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\n      sycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::write_only, sycl::no_init};\n\n        h.parallel_for&lt;VectorAddID&gt;(sycl::range(kVectSize),[=](sycl::id&lt;1&gt; idx) {\n      accessor_c[idx] = accessor_a[idx] + accessor_b[idx];\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = vec_a[i] + vec_b[i];\n      if (vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>If you want to have a fine-grained control of your data-parallel kernel, ND-range data-parallel kernels are the equivalent of ND-range kernels in OpenCL. </li> </ul> <p>ND-range kernel in SYCL</p> <ul> <li><code>nd_range(range&lt;dimensions&gt; globalSize, range&lt;dimensions&gt; localSize);</code></li> <li>ND-range kernels are defined with two range objects<ul> <li>global representing the total size of work-items</li> <li>local representing the size of work-groups</li> </ul> </li> </ul> <p>Tiled Matrix Multiplication</p> QuestionSolution <ul> <li>Fill the blank and complete the code  <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\n\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    // initialize input and output memory on the host\n    constexpr size_t N = 512;\n    constexpr size_t B =  16;\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); \n\n    std::random_device rd;\n    std::mt19937 mt(rd());\n    std::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n\n    // Generate random values\n    std::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // Generate random values\n    std::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // fill with zero\n    std::fill(mat_c.begin(), mat_c.end(), 0.0); \n\n\n    std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      // We can access the buffer using mat[i][j]\n      sycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\n\n\n      /* DEFINE HERE the global size and local size ranges*/\n\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        sycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\n        sycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\n\n        h.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n\n            [[intel::max_work_group_size(1, B, B)]]    {\n            // Indices in the global index space:\n            int m = item.get_global_id()[0];\n            int n = item.get_global_id()[1];\n\n            // Index in the local index space:\n            // Provide local indexes i and j -- fill here\n\n            float sum = 0;\n            for (int p = 0; p &lt; N/B; p++) {\n              // Load the matrix tile from matrix A, and synchronize\n              // to ensure all work-items have a consistent view\n              // of the matrix tile in local memory.\n              tileA[i][j] = accessor_a[m][p*B+j];\n              // Do the same for tileB\n              // fill here \n              item.barrier();\n\n              // Perform computation using the local memory tile, and\n              // matrix B in global memory.\n              for (int kk = 0; kk &lt; B; kk++) {\n       sum += tileA[i][kk] * tileB[kk][j];\n              }\n\n              // After computation, synchronize again, to ensure all\n         // Fill here \n            }\n\n            // Write the final result to global memory.\n            accessor_c[m][n] = sum;\n\n        });\n      });\n    }\n\n\n  // result is copied back to host automatically when accessors go out of\n  // scope.\n\n    // verify that Matrix multiplication is correct\n    for (int i = 0; i &lt; N; i++) {\n      for (int j = 0; j &lt; N; j++){\n         float true_val=0.0;\n         for (int k = 0 ; k &lt; N; k++){\n           true_val += mat_a[i*N +k] * mat_b[k*N+j];\n         }\n         if (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\n            std::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\n            passed = false;\n         }\n    }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\n#define N 512\n#define B  16\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\n\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    // initialize input and output memory on the host\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); \n\n    std::random_device rd;\n    std::mt19937 mt(rd());\n    std::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n\n    // Generate random values\n    std::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // Generate random values\n    std::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // fill with zero\n    std::fill(mat_c.begin(), mat_c.end(), 0.0); \n\n\n    std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      // We can access the buffer using mat[i][j]\n      sycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\n\n\n      sycl::range global {N,N};\n      sycl::range local  {B,B}; \n\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        sycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\n        sycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\n\n       h.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n\n            [[intel::max_work_group_size(1, B, B)]]    {\n            // Indices in the global index space:\n            int m = item.get_global_id()[0];\n            int n = item.get_global_id()[1];\n\n            // Index in the local index space:\n            int i = item.get_local_id()[0];\n           int j = item.get_local_id()[1];\n\n            float sum = 0;\n            for (int p = 0; p &lt; N/B; p++) {\n              // Load the matrix tile from matrix A, and synchronize\n              // to ensure all work-items have a consistent view\n              // of the matrix tile in local memory.\n              tileA[i][j] = accessor_a[m][p*B+j];\n              tileB[i][j] = accessor_b[p*B+i][n];\n              sycl::group_barrier(item.get_group());\n\n              // Perform computation using the local memory tile, and\n              // matrix B in global memory.\n              for (int kk = 0; kk &lt; B; kk++) {\n               sum += tileA[i][kk] * tileB[kk][j];\n              }\n\n              // After computation, synchronize again, to ensure all\n              // reads from the local memory tile are complete.\n              sycl::group_barrier(item.get_group());\n            }\n\n            // Write the final result to global memory.\n            accessor_c[m][n] = sum;\n\n        });\n      });\n    }\n\n\n  // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that Matrix multiplication is correct\n    for (int i = 0; i &lt; N; i++) {\n   for (int j = 0; j &lt; N; j++){\n      float true_val=0.0;\n      for (int k = 0 ; k &lt; N; k++){\n       true_val += mat_a[i*N +k] * mat_b[k*N+j];\n      }\n          if (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\n               std::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\n               passed = false;\n           }\n   }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <p>Warning on work-items group size</p> <ul> <li>If the attribute [[intel::max_work_group_size(Z, Y, X)]] is not specified in your kernel, the workgroup size assumes a default value depending on compilation time and runtime constraints</li> <li>If your kernel contains a barrier, the Intel\u00ae oneAPI DPC++/C++ Compiler sets a default maximum scalarized work-group size of 128 work-items ==&gt; without this attribute, the previous ND-Range kernel would have failed since we have a local work-group size of B x B = 256 work-items </li> </ul>"},{"location":"writing/#pipelining-with-single-work-item-loop","title":"Pipelining with single-work item (loop)","text":"<ul> <li>When your code can't be decomposed into independent works, you can rely on loop parallelism using FPGA</li> <li>In such a situation, the pipeline inputs is not work-items but loop iterations</li> <li>For single-work-item kernels, the developer does not need to do anything special to preserve the data dependency </li> <li>Communications between kernels is also much easier</li> </ul> DPC++ book -- Figure 17-21  <ul> <li>FPGA can efficiently handle loop execution, often maintaining a fully occupied pipeline or providing reports on what changes are necessary to enhance occupancy.</li> <li>It's evident that if loop iterations were substituted with work-items, where the value created by one work-item would have to be transferred to another for incremental computation, the algorithm's description would become far more complex.</li> </ul> <p>Single-work item creation</p> <ul> <li>Replace the <code>parallel_for</code>method by the <code>single_task</code> method defined in the handler class to create a single-work item kernel</li> <li>The source file <code>vector_add.cpp</code> from <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code> uses loop pipelining.</li> </ul> <pre><code>  #include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n  #include &lt;sycl/sycl.hpp&gt;\n\n  using namespace sycl;\n\n  int main(){\n\n\n  // queue creation &amp; data initialization\n\n\n   q.submit([&amp;](handler &amp;h) {\n     h.single_task&lt;class MyKernel&gt;([=]() {\n       // Code to be executed as a single task\n     });\n   });\n   q.wait();\n  }\n</code></pre>"},{"location":"writing/#task-parallelism","title":"Task parallelism","text":"<p>Pipes function as a first-come, first-served buffer system, linking different parts of a design. The Intel\u00ae oneAPI DPC++/C++ Compiler offers various pipe types:</p> <ul> <li> <p>Host Pipes: These establish a connection between a host and a device.</p> </li> <li> <p>Inter-Kernel Pipes: These facilitate efficient and low-latency data transfer and synchronization between kernels. They enable kernels to interact directly using on-device FIFO buffers, which utilize FPGA memory. The Intel\u00ae oneAPI DPC++/C++ Compiler promotes simultaneous kernel operation. By employing inter-kernel pipes for data movement among concurrently running kernels, data can be transferred without waiting for a kernel to finish, enhancing your design's throughput.</p> </li> <li> <p>I/O Pipes: This is a one-way connection to the hardware, either as a source or sink, which can be linked to an FPGA board's input or output functionalities. Such functionalities could encompass network interfaces, PCIe\u00ae, cameras, or other data acquisition or processing tools and protocols.</p> </li> </ul>"},{"location":"writing/#inter-kernel-pipes","title":"Inter-Kernel Pipes","text":"<ul> <li>We will only focus on Inter-Kernel Pipes to leverage task parallelism</li> <li>As for OpenCL programming, pipes can be blocking or non-blocking</li> <li>For Intel\u00ae oneAPI with FPGA, you need to include FPGA extension: <pre><code>#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n</code></pre></li> </ul> <p>Pipe creation and usage</p> Blocking pipesNon-Blocking pipes <pre><code>// Using alias eases considerably their usage\nusing my_pipe = ext::intel::pipe&lt;      \n                class InterKernelPipe, // An identifier for the pipe.\n                int,                   // The type of data in the pipe.\n                4&gt;;                    // The capacity of the pipe.\n\n// Single_task kernel 1\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_in, h);\n    h.single_task([=]() {\n        for (int i=0; i &lt; count; i++) {\n            my_pipe::write(A[i]); // write a single int into the pipe\n\n        }\n    });\n}); \n\n// Single_task kernel 2\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_out, h);\n    h.single_task([=]() {\n        for (int i=0; i &lt; count; i++) {\n            A[i] = my_pipe::read(); // read the next int from the pipe\n        }\n    });\n}); \n</code></pre> <pre><code>// Using alias eases considerably their usage\nusing my_pipe = ext::intel::pipe&lt;      \n                class InterKernelPipe, // An identifier for the pipe.\n                int,                   // The type of data in the pipe.\n                4&gt;;                    // The capacity of the pipe.\n\n// Single_task kernel 1\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_in, h);\n    h.single_task([=]() {\n        valid_write = false;\n        for (int i=0; i &lt; count; i++) {\n            my_pipe::write(A[i],valid_write); // write a single int into the pipe\n\n        }\n    });\n}); \n\n// Single_task kernel 2\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_out, h);\n    h.single_task([=]() {\n        valid_read = false;\n        for (int i=0; i &lt; count; i++) {\n            A[i] = my_pipe::read(valid_read); // read the next int from the pipe\n        }\n    });\n}); \n</code></pre> <p>Stalling pipes</p> <ul> <li>Care should be taken when implementing pipes, especially when there is a strong imbalance between the consumer kernel reading from the pipe and the producer kernel that feed the pipe. </li> <li>Stalling pipes can be disastrous when using blocking pipes</li> </ul>"},{"location":"writing/#multiple-homogeneous-fpga-devices","title":"Multiple Homogeneous FPGA Devices","text":"<ul> <li> <p>Each Meluxina's FPGA nodes have two FPGA cards</p> </li> <li> <p>You can verify their presence using the following commands: <code>aocl list-devices</code> or <code>sycl-ls</code> </p> </li> <li> <p>Differents kernels or the same kernels can be passed to these devices</p> </li> <li> <p>Each devices should have his own <code>sycl::queue</code> and share or not a same context</p> </li> <li> <p>Intel recommends to use a single context for performance reasons as show below:</p> </li> </ul> <p>Running on the two FPGA cards</p> <pre><code>    ...\n\n    sycl::platform p(selector);\n    auto devices = p.get_devices();\n    sycl::context C(devices);\n    sycl::queue q0 (C, devices[0]);\n    sycl::queue q1 (C, devices[1]);\n\n\n   std::cout &lt;&lt; \"Running on device: \"\n             &lt;&lt; devices[0].get_info&lt;sycl::info::device::name&gt;().c_str()\n             &lt;&lt; std::endl;\n\n   std::cout &lt;&lt; \"Running on device: \"\n             &lt;&lt; devices[1].get_info&lt;sycl::info::device::name&gt;().c_str()\n             &lt;&lt; std::endl;\n\n   ... \n</code></pre>"},{"location":"writing/#multiple-nodes","title":"Multiple nodes","text":"<ul> <li> <p>Meluxina FPGA's partition contains 20 nodes </p> </li> <li> <p>Combining MPI with the SYCL language allows developers to scale applications across diverse platforms within a distributed computing environment.</p> </li> <li> <p>Note that MPI cannot be called inside a kernel </p> </li> <li> <p>FPGA comminucation path :</p> </li> </ul> <pre><code>graph LR\n    A[FPGA 1] --&gt;|PCIe| B[NODE 1];\n    B[NODE 1] --&gt;|Infiniband| C[NODE 2];\n    C[NODE 1] --&gt;|PCIe| D[FPGA 2];</code></pre> <p>MPI Programs Using C++ with SYCL running on  multiple FPGAs</p> <p><pre><code>#include &lt;mpi.h&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;iomanip&gt;  // setprecision library\n#include &lt;iostream&gt;\n#include &lt;numeric&gt; \n\n\nusing namespace sycl;\nconstexpr int master = 0;\n\n////////////////////////////////////////////////////////////////////////\n//\n// Each MPI ranks compute the number Pi partially on target device using SYCL.\n// The partial result of number Pi is returned in \"results\".\n//\n////////////////////////////////////////////////////////////////////////\nvoid mpi_native(double* results, int rank_num, int num_procs,\n                long total_num_steps, queue&amp; q) {\n\n  double dx = 1.0f / (double)total_num_steps;\n  long items_per_proc = total_num_steps / size_t(num_procs);\n  // The size of amount of memory that will be given to the buffer.\n  //range&lt;1&gt; num_items{items_per_proc};\n\n  // Buffers are used to tell SYCL which data will be shared between the host\n  // and the devices.\n  buffer&lt;double, 1&gt; results_buf(results,\n                               range&lt;1&gt;(items_per_proc));\n\n  // Submit takes in a lambda that is passed in a command group handler\n  // constructed at runtime.\n  q.submit([&amp;](handler&amp; h) {\n    // Accessors are used to get access to the memory owned by the buffers.\n    accessor results_accessor(results_buf,h,write_only);\n    // Each kernel calculates a partial of the number Pi in parallel.\n    h.parallel_for(range&lt;1&gt;(items_per_proc), [=](id&lt;1&gt; k) {\n      double x = ((double)(rank_num * items_per_proc + k))  * dx ;\n      results_accessor[k] = (4.0f * dx) / (1.0f + x * x);\n    });\n  });\n}\n\n\nint main(int argc, char** argv) {\n  long num_steps = 1000000;\n  char machine_name[MPI_MAX_PROCESSOR_NAME];\n  int name_len=0;\n  int id=0;\n  int num_procs=0;\n  double pi=0.0;\n  double t1, t2;\n  try {\n  // Use compile-time macros to select either:\n  //   - the FPGA emulator device (CPU emulation of the FPGA)\n  //   - the FPGA device (a real FPGA)\n  //   - the simulator device\n  #if FPGA_SIMULATOR\n  auto selector = ext::intel::fpga_simulator_selector_v;\n  #elif FPGA_HARDWARE\n  auto selector = ext::intel::fpga_selector_v;\n  #elif FPGA_EMULATOR\n  auto selector = ext::intel::fpga_emulator_selector_v;\n  #else \n  auto selector = sycl::cpu_selector_v;\n  #endif\n\n  property_list q_prop{property::queue::in_order()};\n  queue myQueue{selector,q_prop};\n\n  // Start MPI.\n  if (MPI_Init(&amp;argc, &amp;argv) != MPI_SUCCESS) {\n    std::cout &lt;&lt; \"Failed to initialize MPI\\n\";\n    exit(-1);\n  }\n\n  // Create the communicator, and retrieve the number of MPI ranks.\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;num_procs);\n\n  // Determine the rank number.\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;id);\n\n  // Get the machine name.\n  MPI_Get_processor_name(machine_name, &amp;name_len);\n\n  if(id == master) t1 = MPI_Wtime();\n\n  std::cout &lt;&lt; \"Rank #\" &lt;&lt; id &lt;&lt; \" runs on: \" &lt;&lt; machine_name\n            &lt;&lt; \", uses device: \"\n            &lt;&lt; myQueue.get_device().get_info&lt;info::device::name&gt;() &lt;&lt; \"\\n\";\n\n  int num_step_per_rank = num_steps / num_procs;\n  double* results_per_rank = new double[num_step_per_rank];\n\n  // Initialize an array to store a partial result per rank.\n  for (size_t i = 0; i &lt; num_step_per_rank; i++) results_per_rank[i] = 0.0;\n\n  // Calculate the Pi number partially by multiple MPI ranks.\n  mpi_native(results_per_rank, id, num_procs, num_steps, myQueue);\n\n  double local_sum = 0.0;\n  for(unsigned int i = 0; i &lt; num_step_per_rank; i++){\n    local_sum += results_per_rank[i];\n  }\n\n  // Master rank performs a reduce operation to get the sum of all partial Pi.\n  MPI_Reduce(&amp;local_sum, &amp;pi, 1, MPI_DOUBLE, MPI_SUM, master, MPI_COMM_WORLD);\n\n  if (id == master) {\n    t2 = MPI_Wtime(); \n    std::cout &lt;&lt; \"mpi native:\\t\\t\";\n    std::cout &lt;&lt; std::setprecision(10) &lt;&lt; \"PI =\" &lt;&lt; pi &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Elapsed time is \" &lt;&lt; t2-t1 &lt;&lt; std::endl;\n  }\n\n  delete[] results_per_rank;\n\n  MPI_Finalize();\n\n } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n\n  return 0;\n}\n</code></pre>  Output : <pre><code>Rank #3 runs on: mel3014, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nRank #0 runs on: mel3001, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nRank #4 runs on: mel3017, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nRank #2 runs on: mel3013, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nRank #1 runs on: mel3010, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nmpi native:             PI =3.141593654\nElapsed time is 9.703053059\n</code></pre></p>"}]}